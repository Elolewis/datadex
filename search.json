[
  {
    "objectID": "notebooks/climate.html",
    "href": "notebooks/climate.html",
    "title": "Climate",
    "section": "",
    "text": "Evolution of the CO2 concentration in the atmosphere.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\ndf = pd.read_parquet(\"../data/climate_co2_global_trend.parquet\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Altair interactive line plot\nc = (\n    alt.Chart(df)\n    .mark_line()\n    .encode(\n        x=\"date:T\",\n        y=alt.Y(\"trend:Q\", scale=alt.Scale(domain=[390, 430]), title=\"CO2 (ppm)\"),\n        tooltip=[\"date:T\", \"trend:Q\"],\n    )\n    .properties(width=\"container\", height=200)\n    .configure_view(strokeWidth=0)\n    .configure_axis(grid=False, labelFontSize=14, titleFontSize=14)\n    .configure_title(fontSize=16, anchor=\"start\", color=\"gray\")\n    .display()\n)"
  },
  {
    "objectID": "notebooks/duckdb-ipfs.html",
    "href": "notebooks/duckdb-ipfs.html",
    "title": "Making DuckDB understand IPFS hashes",
    "section": "",
    "text": "Turns out you can query arbitrary filesystems with SQL. This notebook shows how to query IPFS with DuckDB.\nThe gist of it is that you can register any fsspec filesystem on DuckDB. As IPFS is supported by fsspec via ipfsspec, we can register it and query it with SQL.\nBefore running this code, you‚Äôll need to install ipfsspec. You can do so with:\npip install git+https://github.com/fsspec/ipfsspec\n\nimport duckdb\nfrom ipfsspec import AsyncIPFSFileSystem\n\nipfs_fs = AsyncIPFSFileSystem()\n\nduckdb.register_filesystem(ipfs_fs)\n\nOnce the filesystem is registered, you can use specific URIs inside read_csv_auto or read_parquet!\nA couple of examples:\n\n_ = (\n    duckdb.sql(\n        \"select * from read_csv_auto('ipfs://bafybeif5reawvqtsoybj5fhdl4ghaq3oc7kzepuws26zawkjm4johlv3uq')\"\n    )\n    .df()\n    .groupby(\"kingdom_name\")[\"taxonid\"]\n    .count()\n    .plot.bar()\n)\n\n\n\n\n\n_ = (\n    duckdb.query(\n        \"select * from read_parquet('ipfs://bafkreibnx5q6qwxobozkdm6xt7ktvwciyfvtkgy7fud67w5oyxnf5tch4e') limit 10\"\n    )\n    .df()\n    .groupby(\"year\")\n    .mean(\"literacy_rate\")\n    .plot()\n)"
  },
  {
    "objectID": "notebooks/quarto.html",
    "href": "notebooks/quarto.html",
    "title": "Quarto Notebooks",
    "section": "",
    "text": "This document contains a few examples of how to do things in Quarto‚Äôs world. All is coming from a Jupyter Notebook fully integrated with Datadex!\nWe can display dataframes as tables:\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ntable = [\n    [\"Sun\", 696000, 1989100000],\n    [\"Earth\", 6371, 5973.6],\n    [\"Moon\", 1737, 73.5],\n    [\"Mars\", 3390, 641.85],\n]\nMarkdown(tabulate(table, headers=[\"Planet\", \"R (km)\", \"mass (x 10^29 kg)\"]))\n\n\n\nTable¬†1: Planets\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\nWe can do basic plots:\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 23, 2, 4])\nplt.show()\n\nplt.plot([8, 65, 23, 90])\nplt.show()\n\nINFO: generated new fontManager\n\n\n\n\n\n\n\n\n(a) First\n\n\n\n\n\n\n\n(b) Second\n\n\n\n\nFigure¬†1: Charts\n\n\n\nBut also dynamic graphs:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nsource = pd.DataFrame(\n    np.cumsum(np.random.randn(100, 3), 0).round(2),\n    columns=[\"A\", \"B\", \"C\"],\n    index=pd.RangeIndex(100, name=\"x\"),\n)\nsource = source.reset_index().melt(\"x\", var_name=\"category\", value_name=\"y\")\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection_point(nearest=True, on=\"mouseover\", fields=[\"x\"], empty=False)\n\n# The basic line\nline = (\n    alt.Chart(source)\n    .mark_line(interpolate=\"basis\")\n    .encode(x=\"x:Q\", y=\"y:Q\", color=\"category:N\")\n)\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = (\n    alt.Chart(source)\n    .mark_point()\n    .encode(\n        x=\"x:Q\",\n        opacity=alt.value(0),\n    )\n    .add_params(nearest)\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align=\"left\", dx=5, dy=-5).encode(\n    text=alt.condition(nearest, \"y:Q\", alt.value(\" \"))\n)\n\n# Draw a rule at the location of the selection\nrules = (\n    alt.Chart(source)\n    .mark_rule(color=\"gray\")\n    .encode(\n        x=\"x:Q\",\n    )\n    .transform_filter(nearest)\n)\n\n# Put the five layers into a chart and bind the data\nalt.layer(line, selectors, points, rules, text).properties(\n    width=\"container\", height=300\n)\n\n\n\n\n\n\n\nWe can even embed a whole dataset and interact with it Tabelau style.\n\nimport pygwalker as pyg\n\npyg.walk(source)\n\n\n\n\n\n\n\nNotice you can drag and drop columns to change the graph to your heart‚Äôs content. A¬∑we¬∑some!"
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "README.html#usage",
    "href": "README.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "README.html#what-can-you-do-with-datadex",
    "href": "README.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "README.html#setup",
    "href": "README.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "README.html#motivation",
    "href": "README.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "README.html#acknowledgements",
    "href": "README.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  },
  {
    "objectID": "notebooks/pyscript.html",
    "href": "notebooks/pyscript.html",
    "title": "PyScript",
    "section": "",
    "text": "[splashscreen]\n        enabled = false\nSo‚Ä¶ this is the current date and time, computed by Python running in your browser!\nfrom datetime import datetime; now = datetime.now(); display(now.strftime(\"%m/%d/%Y, %H:%M:%S\"));"
  },
  {
    "objectID": "notebooks/pyscript.html#jupyter-repl",
    "href": "notebooks/pyscript.html#jupyter-repl",
    "title": "PyScript",
    "section": "Jupyter REPL",
    "text": "Jupyter REPL\nYou can also have an entire Jupyter REPL running in your browser, with the ability to run arbitrary Python code!\nGo ahead and try this out:\nfrom matplotlib import pyplot as plt\nimport numpy as np\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale) * 500)\nax.set(title=\"Some random data!\")\nplt.show()"
  },
  {
    "objectID": "notebooks/brlda.html",
    "href": "notebooks/brlda.html",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "",
    "text": "Let‚Äôs start by loading the data.\nimport pandas as pd\n\nemployees_df = pd.read_csv(\"../data/employees.csv\")\nsafehouses_df = pd.read_csv(\"../data/safehouses.csv\")\ndivisions_df = pd.read_csv(\"../data/divisions.csv\")\nmanagers_df = pd.read_csv(\"../data/managers.csv\")\nactions_df = pd.read_csv(\"../data/actions.csv\")"
  },
  {
    "objectID": "notebooks/brlda.html#employees",
    "href": "notebooks/brlda.html#employees",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Employees",
    "text": "Employees\n\nemployees_df.sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n512\n513\nChristopher Mckinney\nData Scientist\nchristopher_mckinney@brlda.gov\n\\t+1-212-758-3594x7701\nMrs. Michelle James DDS\n\n\n24693\n24698\nAndrew Fleming\nMachine Learning Engineer\nandrew_fleming@brlda.gov\n(083)562-9918\nMatthew Joseph\n\n\n6804\n6807\nOmar Chavez\nData Analyst\nomar_chavez@brlda.gov\n079.884.2902\nMrs. Lynn Owens MD\n\n\n12078\n12081\nTimothy Cohen\nBusiness Analyst\ntimothy_cohen@brlda.gov\n\\t+1-606-026-0666x1798\nAshley Marshall\n\n\n7650\n7653\nAustin Blair\nData Engineer\naustin_blair@brlda.gov\n865.692.3918\nLee Herrera DVM\n\n\n\n\n\n\n\n\nemployees_df[\"EmployeeName\"].value_counts().head(5)\n\nEmployeeName\nMichael Smith       12\nDavid Smith         12\nLisa Smith          11\nJohn Smith          11\nMichael Williams    10\nName: count, dtype: int64\n\n\nSome duplicate employees are present in the data‚Ä¶\n\nemployees_df[employees_df[\"EmployeeName\"] == \"Michael Smith\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n4448\n4451\nMichael Smith\nProject Manager\nmichael_smith@brlda.gov\n001-137-479-2502x05426\nNathan Guerra\n\n\n5129\n5132\nMichael Smith\nScrum Master\nmichael_smith@brlda.gov\n223-274-6121x5268\nDavid Houston\n\n\n7455\n7458\nMichael Smith\nQuality Assurance Analyst\nmichael_smith@brlda.gov\n(202)737-6861x908\nPamela Hale\n\n\n10672\n10675\nMichael Smith\nData Scientist\nmichael_smith@brlda.gov\n001-192-379-3454x580\nSarah Brown\n\n\n11683\n11686\nMichael Smith\nQuality Assurance Analyst\nmichael_smith@brlda.gov\n001-038-692-4506x839\nPatrick Cruz\n\n\n13957\n13960\nMichael Smith\nProgram Manager\nmichael_smith@brlda.gov\n(471)896-2210x7208\nHeather Crawford\n\n\n15668\n15672\nMichael Smith\nScrum Master\nmichael_smith@brlda.gov\n668.014.9966x33380\nGregory Floyd\n\n\n17442\n17446\nMichael Smith\nData Analyst\nmichael_smith@brlda.gov\n\\t+1-544-294-4533x69316\nRichard Collins\n\n\n17570\n17574\nMichael Smith\nProgram Manager\nmichael_smith@brlda.gov\n9730334254\nMichele Graham\n\n\n19322\n19326\nMichael Smith\nMachine Learning Engineer\nmichael_smith@brlda.gov\n081-057-7739\nJoshua James\n\n\n22603\n22608\nMichael Smith\nProgram Manager\nmichael_smith@brlda.gov\n001-714-897-8059x3979\nAmber Garcia\n\n\n23884\n23889\nMichael Smith\nProject Manager\nmichael_smith@brlda.gov\n\\t+1-800-158-9222\nMichael Harris\n\n\n\n\n\n\n\nAre they all the same employee? The email match but the ID and Phone don‚Äôt.\nLet‚Äôs focus on the missing data. If they removed their users, we should see some missing EmployeeID values.\n\n# Check min and max EmployeeID\nmin_eid = employees_df[\"EmployeeID\"].min()\nmax_eid = employees_df[\"EmployeeID\"].max()\nprint(\"Min EmployeeID: \", min_eid)\nprint(\"Max EmployeeID: \", max_eid)\n\n# Check employees ids not in the min-max range\nmissing_employees = set(range(min_eid, max_eid + 1)) - set(employees_df[\"EmployeeID\"])\nprint(\"Employees ids not in the min-max range: \", missing_employees)\n\nMin EmployeeID:  1\nMax EmployeeID:  26849\nEmployees ids not in the min-max range:  {14976, 22602, 26188, 1423, 4284}\n\n\nThese might be the ‚Äúghosts‚Äù agents. Let‚Äôs quickly check that all managers are also employees just in case‚Ä¶\n\nmanagers = set(employees_df[\"Manager\"])\nemployees = set(employees_df[\"EmployeeName\"])\n\nmanagers - employees\n\nset()\n\n\nAre these ‚Äúghosts employees‚Äù in other tables?\n\ndivisions_df[divisions_df[\"EmployeeID\"].isin(missing_employees)]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n1422\n1423\nNaN\n[Division 7]\n[Project e-enable_holistic_models]\n[14, 214, 181, 219]\n\n\n4283\n4284\nNaN\n[Division 7]\n[Project repurpose_collaborative_methodologies...\n[10, 219]\n\n\n14975\n14976\nNaN\n[Division 7]\n[Project transform_24/365_functionalities]\n[25, 154, 231, 33, 219]\n\n\n22601\n22602\nNaN\n[Division 7]\n[Project monetize_one-to-one_mindshare, Projec...\n[12, 221, 19, 18, 219]\n\n\n26187\n26188\nNaN\n[Division 7]\n[Project extend_robust_action-items]\n[7, 219]\n\n\n\n\n\n\n\nThey all belong to Division 7. What actions did they take?\n\nactions_df[actions_df[\"EmployeeID\"].isin(missing_employees)].sort_values([\"ActionDate\"])\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\n\n\n\n\n41824\n14976\nQuantum Key Generation\n1994-06-06 00:00:00\nperform data mining on social media data for s...\nPuerto Rico\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n53036\n26188\nPredictive Modeling\n1994-11-20 00:00:00\nconstruct algorithms for automatic gait recogn...\nPuerto Rico\nfailed\ncritical\nProject extend_robust_action-items\nDivision 10\n\n\n4283\n4284\nData Clustering\n1996-05-08 00:00:00\nInitiate operation Networked_discrete_system_e...\nMartinique\ncompleted\nhigh\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n81969\n1423\nUser Profiling\n1997-04-12 00:00:00\nOperation Re-contextualized_attitude-oriented_...\nEgypt\nfailed\nmedium\nProject e-enable_holistic_models\nDivision 3\n\n\n68673\n14976\nNatural Language Generation\n2007-06-06 00:00:00\nconstruct algorithms for automatic vein recogn...\nBenin\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n31132\n4284\nQuantum Resistant Cryptography\n2007-09-17 00:00:00\nInitiate operation Customizable_discrete_paral...\nKazakhstan\nfailed\ncritical\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n57981\n4284\nMachine Learning-based Intrusion Detection\n2007-10-25 00:00:00\nanalyze communication patterns through Fully-c...\nAlbania\ncompleted\nhigh\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n28271\n1423\nAutomated Surveillance\n2009-10-18 00:00:00\nOperation Down-sized_24/7_capability to develo...\nPuerto Rico\nfailed\nhigh\nProject e-enable_holistic_models\nDivision 3\n\n\n14975\n14976\nNatural Language Generation\n2011-08-06 00:00:00\nInitiate operation Centralized_upward-trending...\nBahrain\ncompleted\nlow\nProject transform_24/365_functionalities\nDivision 1\n\n\n49450\n22602\nQuantum Key Generation\n2012-10-22 00:00:00\nOperation Digitized_methodical_structure to ap...\nIreland\nfailed\nhigh\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n55120\n1423\nData Leakage Detection\n2014-05-29 00:00:00\nInitiate operation Sharable_impactful_core, ta...\nAntarctica (the territory South of 60 deg S)\ncompleted\nlow\nProject e-enable_holistic_models\nDivision 3\n\n\n76299\n22602\nCryptocurrency Tracing\n2014-08-08 00:00:00\nanalyze network topology for vulnerabilities t...\nSan Marino\ncompleted\nmedium\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n95522\n14976\nPattern-based Malware Detection\n2015-12-12 00:00:00\nutilize machine learning for anomaly detection...\nCzech Republic\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n26187\n26188\nVoice Recognition\n2017-10-19 00:00:00\nperform text mining on classified documents th...\nBangladesh\nfailed\nmedium\nProject extend_robust_action-items\nDivision 10\n\n\n1422\n1423\nVideo Analysis\n2018-03-01 00:00:00\nInitiate operation Digitized_dedicated_help-de...\nUruguay\nfailed\nlow\nProject e-enable_holistic_models\nDivision 3\n\n\n22601\n22602\nCovert Behavioral Analytics\n2019-01-30 00:00:00\nOperation Vision-oriented_explicit_flexibility...\nIreland\nfailed\nmedium\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n79885\n26188\nData Extraction\n2020-12-01 00:00:00\nInitiate operation Realigned_exuding_Graphic_I...\nUnited States Minor Outlying Islands\nin progress\nlow\nProject extend_robust_action-items\nDivision 10\n\n\n84830\n4284\nCovert Social Network Analysis\n2021-03-30 00:00:00\nInitiate operation Self-enabling_tangible_webs...\nMexico\ncompleted\nlow\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n\n\n\n\n\nSome interesting things: - They all mention the devices in their action description. - They belong to a different Associated Division and not Division 7.\nHow common is for employees to do actions in other divisions? Time for OBT (one big table).\n\ndetailed_actions_df = actions_df.merge(\n    employees_df, left_on=\"EmployeeID\", right_on=\"EmployeeID\", how=\"left\"\n).merge(divisions_df, left_on=\"EmployeeID\", right_on=\"EmployeeID\", how=\"left\")\ndetailed_actions_df.sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\nEmployeeName_x\nJobTitle\nEmail\nPhone\nManager\nEmployeeName_y\nDivision\nProject\nknown_safehouses\n\n\n\n\n51345\n24497\nData Clustering\n2004-10-18 00:00:00\nInitiate operation Decentralized_holistic_defi...\nCosta Rica\ncompleted\nhigh\nProject extend_robust_action-items\nDivision 1\nJonathan Day\nData Analyst\njonathan_day@brlda.gov\n931.043.2180x8210\nLuis Mccoy\nJonathan Day\n[Division 1, Division 2]\n[Project strategize_value-added_bandwidth, Pro...\n[211]\n\n\n79376\n25679\nNatural Language Processing\n2000-01-30 00:00:00\nOperation Proactive_maximized_groupware to con...\nIran\nfailed\nhigh\nProject drive_value-added_mindshare\nDivision 4\nAnthony Jackson\nData Scientist\nanthony_jackson@brlda.gov\n001-502-237-1029\nMaria Vazquez DVM\nAnthony Jackson\n[Division 4, Division 6, Division 4]\n[Project unleash_front-end_models, Project dri...\n[35, 48]\n\n\n34640\n7792\nImage Analysis\n1997-09-01 00:00:00\nperform sentiment analysis on customer reviews...\nCambodia\nfailed\nlow\nProject deliver_visionary_web-readiness\nDivision 9\nBradley Davis\nStatistician\nbradley_davis@brlda.gov\n3517150454\nAaron Todd\nBradley Davis\n[Division 9]\n[Project deliver_visionary_web-readiness, Proj...\n[45, 55]\n\n\n82486\n1940\nQuantum Computing Decoding\n2018-08-25 00:00:00\nOperation Quality-focused_eco-centric_time-fra...\nIsrael\nfailed\ncritical\nProject mesh_cutting-edge_experiences\nDivision 7\nJohn Thompson\nBusiness Analyst\njohn_thompson@brlda.gov\n\\t+1-462-677-1117x520\nAngela Carlson\nJohn Thompson\n[Division 8, Division 7]\n[Project mesh_cutting-edge_experiences]\n[133, 193]\n\n\n36370\n9522\nDDoS Attack Detection\n2007-03-23 00:00:00\nOperation Public-key_systematic_methodology to...\nSierra Leone\ncompleted\nmedium\nProject scale_back-end_interfaces\nDivision 9\nJustin Young\nData Scientist\njustin_young@brlda.gov\n858.613.0262\nThomas Stewart\nJustin Young\n[Division 9, Division 2, Division 5]\n[Project scale_back-end_interfaces]\n[67, 2, 49, 142]\n\n\n\n\n\n\n\n\ndef check_division_associated_division(row):\n    return row[\"AssociatedDivision\"] in row[\"Division\"]\n\n\ndetailed_actions_df[\n    ~detailed_actions_df.apply(check_division_associated_division, axis=1)\n].sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\nEmployeeName_x\nJobTitle\nEmail\nPhone\nManager\nEmployeeName_y\nDivision\nProject\nknown_safehouses\n\n\n\n\n92138\n11592\nData Anonymization\n2006-11-18 00:00:00\nanalyze network logs for detecting command and...\nAndorra\ncompleted\ncritical\nProject mesh_cutting-edge_experiences\nDivision 7\nGabriel Smith\nData Scientist\ngabriel_smith@brlda.gov\n596.995.7536x22790\nDebra Gonzalez\nGabriel Smith\n[Division 1, Division 3]\n[Project mesh_cutting-edge_experiences]\n[219, 5, 133]\n\n\n40147\n13299\nCustomer Support Interaction Sentiment Analysis\n2015-05-19 00:00:00\nOperation Cloned_didactic_implementation to ex...\nBenin\nfailed\nlow\nProject embrace_magnetic_systems\nDivision 7\nDillon Moran\nQuality Assurance Analyst\ndillon_moran@brlda.gov\n001-124-430-5696x9265\nNicholas Davila\nDillon Moran\n[Division 3, Division 2, Division 9]\n[Project embrace_magnetic_systems, Project e-e...\n[140, 4, 1]\n\n\n87019\n6473\nSocial Media Post Analysis\n2017-12-25 00:00:00\nestablish covert presence through Optimized_st...\nSlovenia\nfailed\nmedium\nProject embrace_transparent_networks\nDivision 7\nMelissa Thompson\nMachine Learning Engineer\nmelissa_thompson@brlda.gov\n001-896-224-3290\nDavid Rogers\nMelissa Thompson\n[Division 3, Division 5, Division 3]\n[Project embrace_transparent_networks]\n[157, 231, 51, 232]\n\n\n60427\n6730\nMalicious Code Detection\n1999-06-11 00:00:00\nInitiate operation Optional_multimedia_paralle...\nVenezuela\ncompleted\nhigh\nProject facilitate_mission-critical_ROI\nDivision 7\nMichael Duncan\nMachine Learning Engineer\nmichael_duncan@brlda.gov\n\\t+1-112-045-6111x9914\nAlexander Mccullough\nMichael Duncan\n[Division 8, Division 10, Division 6]\n[Project facilitate_mission-critical_ROI]\n[188, 19, 141]\n\n\n75981\n22284\nCovert Sentiment-based Social Engineering\n2018-06-19 00:00:00\nperform sentiment analysis on online forums th...\nWallis and Futuna\nfailed\ncritical\nProject synergize_B2C_technologies\nDivision 7\nElizabeth Garcia\nBusiness Analyst\nelizabeth_garcia@brlda.gov\n234.106.8187x788\nPaul Robertson\nElizabeth Garcia\n[Division 9, Division 5, Division 4]\n[Project synergize_B2C_technologies]\n[220, 151]\n\n\n\n\n\n\n\nSome people did actions associated to Division 7 but is not reflected in the Employee table Divisions.\n\ndetailed_actions_df[\n    ~detailed_actions_df.apply(check_division_associated_division, axis=1)\n][\"AssociatedDivision\"].value_counts()\n\nAssociatedDivision\nDivision 7     2284\nDivision 6        7\nDivision 3        4\nDivision 1        4\nDivision 10       3\nName: count, dtype: int64\n\n\nNow, most of the people that did actions in Division 7 are missclassified, but there are some that did actions in the divisions 1, 3, 6, and 10. These are our suspects."
  },
  {
    "objectID": "notebooks/brlda.html#safehouses",
    "href": "notebooks/brlda.html#safehouses",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Safehouses",
    "text": "Safehouses\n\nsafehouses_df.sample(5)\n\n\n\n\n\n\n\n\nID\nCity\nAddress\nLatitude\nLongitude\n\n\n\n\n198\n1\nNew York\n87 Beekman Avenue, Vernon Park, City of Mount ...\n40.908226\n-73.823729\n\n\n186\n6\nLondon\n20 Robson Avenue, London, NW10 3SZ, United Kin...\n51.543403\n-0.236712\n\n\n34\n183\nLima\nAvenida Coronel N√©stor Gambetta, 200 Millas, C...\n-12.004199\n-77.124531\n\n\n81\n99\nSanta Cruz de la Sierra\nRN9: La Guardia-Abap√≥, Municipio La Guardia, B...\n-17.902468\n-63.252284\n\n\n187\n6\nParis\n53 Rue Victor Hugo, 95200 Sarcelles, France\n49.001127\n2.385609\n\n\n\n\n\n\n\n\n# Map of safehouses with Latitude and Longitude\nimport folium\n\nsafehouses_map = folium.Map(\n    location=[safehouses_df[\"Latitude\"].mean(), safehouses_df[\"Longitude\"].mean()],\n    zoom_start=4,\n)\n\nfor index, row in safehouses_df.iterrows():\n    folium.Marker([row[\"Latitude\"], row[\"Longitude\"]], popup=row[\"ID\"]).add_to(\n        safehouses_map\n    )\n\nsafehouses_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nCute, but not very useful for now, let‚Äôs explore divisions."
  },
  {
    "objectID": "notebooks/brlda.html#divisions",
    "href": "notebooks/brlda.html#divisions",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Divisions",
    "text": "Divisions\n\ndivisions_df\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n0\n1\nKelly Rios\n[Division 1]\n[Project deliver_visionary_web-readiness]\n[232, 1, 73, 217]\n\n\n1\n2\nMadison Barr\n[Division 6, Division 3]\n[Project repurpose_collaborative_methodologies]\n[192, 26, 118, 4]\n\n\n2\n3\nSue Anderson\n[Division 5, Division 1, Division 10]\n[Project repurpose_collaborative_methodologies]\n[19, 8, 130, 50]\n\n\n3\n4\nLaura Carlson\n[Division 9, Division 9, Division 2]\n[Project streamline_proactive_e-markets]\n[15, 232]\n\n\n4\n5\nCarrie Ali\n[Division 3, Division 6]\n[Project deliver_visionary_web-readiness]\n[158, 118]\n\n\n...\n...\n...\n...\n...\n...\n\n\n26844\n26845\nChristopher Riley\n[Division 8, Division 10, Division 5]\n[Project drive_value-added_mindshare, Project ...\n[226]\n\n\n26845\n26846\nEric Chan\n[Division 2, Division 6, Division 1]\n[Project deliver_visionary_web-readiness]\n[15, 99, 58]\n\n\n26846\n26847\nAmy Vazquez\n[Division 8, Division 10, Division 5]\n[Project embrace_transparent_networks, Project...\n[22]\n\n\n26847\n26848\nClifford Reyes\n[Division 9, Division 2, Division 6]\n[Project disintermediate_distributed_experienc...\n[98]\n\n\n26848\n26849\nNoah Snyder\n[Division 2, Division 9, Division 10]\n[Project strategize_value-added_bandwidth]\n[1]\n\n\n\n\n26849 rows √ó 5 columns\n\n\n\n\n# TODO: Explode Division and Project\n\n\n# TODO: Some divisions are repeated in Division\n\n\nManagers\n\nmanagers_df.sample(5)\n\n\n\n\n\n\n\n\nManagerName\nEmployee_1\nEmployee_2\nEmployee_3\nEmployee_4\nEmployee_5\nEmployee_6\nEmployee_7\nEmployee_8\nEmployee_9\n...\nEmployee_20\nEmployee_21\nEmployee_22\nEmployee_23\nEmployee_24\nEmployee_25\nEmployee_26\nEmployee_27\nEmployee_28\nEmployee_29\n\n\n\n\n3489\nNancy Fleming\nPaul Shannon\nAdrian Lopez\nNicholas Taylor\nDavid Mcbride\nMr. Jerry Williams II\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n58\nTeresa Johnson\nNathan Gibson\nThomas Jones MD\nAdam Stark\nMartha Sanchez\nMonique Campbell\nCarla Hall\nWilliam Harrington\nAnthony Paul\nLuke Mills\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2060\nDavid Hubbard\nMary Miller\nDenise Williams\nMichael Grant\nAlisha Turner\nAmy Soto\nKrista Walker\nJasmine Ramirez\nJennifer Mills\nSteve Krause\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3126\nJoshua Allen\nJenna Shaffer\nChristopher Stewart\nHeather Ochoa\nBriana Sanchez\nKatherine Adams\nKatherine Kirk\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n204\nNicole Williams\nLucas Campos\nDavid James\nTracy Franklin\nDiana Zimmerman\nMichelle Anderson\nBrett Johnson\nCassandra Gallegos\nTanner Thompson\nJeremy Hopkins\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 30 columns\n\n\n\n\n# Transform to long format\nclean_managers_df = (\n    managers_df.melt(\n        id_vars=[\"ManagerName\"], value_name=\"EmployeeName\", var_name=\"EmployeeNumber\"\n    )\n    .dropna()\n    .drop(columns=[\"EmployeeNumber\"])\n)\n\nclean_managers_df.sample(5)\n\n\n\n\n\n\n\n\nManagerName\nEmployeeName\n\n\n\n\n28030\nJeffery Jones\nMichael Stevens\n\n\n6700\nAlexandra Gibson\nErica Duran\n\n\n1289\nSara Wilcox\nKimberly Webb\n\n\n41082\nBrittany Olson\nKimberly Rollins\n\n\n6816\nMonica Evans\nPatricia Rollins\n\n\n\n\n\n\n\n\nmanagers_df[managers_df[\"Employee_1\"].isna()]\n\n\n\n\n\n\n\n\nManagerName\nEmployee_1\nEmployee_2\nEmployee_3\nEmployee_4\nEmployee_5\nEmployee_6\nEmployee_7\nEmployee_8\nEmployee_9\n...\nEmployee_20\nEmployee_21\nEmployee_22\nEmployee_23\nEmployee_24\nEmployee_25\nEmployee_26\nEmployee_27\nEmployee_28\nEmployee_29\n\n\n\n\n3686\nJessica Stone\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1 rows √ó 30 columns\n\n\n\nJessica Stone is the only manager without any employees (unless there are NaNs gaps).\n\nclean_managers_df[clean_managers_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nManagerName\nEmployeeName\n\n\n\n\n17210\nChristopher Mckenzie\nJessica Stone\n\n\n\n\n\n\n\n\nemployees_df[employees_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n11560\n11563\nJessica Stone\nData Scientist\njessica_stone@brlda.gov\n001-234-563-9331\nChristopher Mckenzie\n\n\n\n\n\n\n\n\ndivisions_df[divisions_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n11562\n11563\nJessica Stone\n[Division 4]\n[Project mesh_cutting-edge_experiences, Projec...\n[18, 42, 44]\n\n\n\n\n\n\n\n\nactions_df[actions_df[\"EmployeeID\"] == 11563]\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\n\n\n\n\n11562\n11563\nGesture Recognition\n2016-06-21 00:00:00\nbuild systems for automatic object detection i...\nSyrian Arab Republic\nfailed\ncritical\nProject unleash_front-end_models\nDivision 4\n\n\n38411\n11563\nAutomated Social Media Profiling\n2012-08-31 00:00:00\nOperation Optimized_real-time_artificial_intel...\nBotswana\nfailed\nlow\nProject unleash_front-end_models\nDivision 4\n\n\n65260\n11563\nObject Recognition\n1996-05-13 00:00:00\nperform data mining on financial transactions ...\nCongo\ncompleted\nmedium\nProject unleash_front-end_models\nDivision 4\n\n\n92109\n11563\nCovert Facial Recognition\n1999-05-26 00:00:00\nInitiate operation Visionary_coherent_architec...\nItaly\ncompleted\ncritical\nProject unleash_front-end_models\nDivision 4"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "index.html#what-can-you-do-with-datadex",
    "href": "index.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  }
]