[
  {
    "objectID": "notebooks/quarto.html",
    "href": "notebooks/quarto.html",
    "title": "Quarto Notebooks",
    "section": "",
    "text": "This document contains a few examples of how to do things in Quarto‚Äôs world. All is coming from a Jupyter Notebook fully integrated with Datadex!\nWe can display dataframes as tables:\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ntable = [\n    [\"Sun\", 696000, 1989100000],\n    [\"Earth\", 6371, 5973.6],\n    [\"Moon\", 1737, 73.5],\n    [\"Mars\", 3390, 641.85],\n]\nMarkdown(tabulate(table, headers=[\"Planet\", \"R (km)\", \"mass (x 10^29 kg)\"]))\n\n\n\nTable¬†1: Planets\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\nWe can do basic plots:\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 23, 2, 4])\nplt.show()\n\nplt.plot([8, 65, 23, 90])\nplt.show()\n\nINFO: generated new fontManager\n\n\n\n\n\n\n\n\n(a) First\n\n\n\n\n\n\n\n(b) Second\n\n\n\n\nFigure¬†1: Charts\n\n\n\nBut also dynamic graphs:\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nsource = pd.DataFrame(\n    np.cumsum(np.random.randn(100, 3), 0).round(2),\n    columns=[\"A\", \"B\", \"C\"],\n    index=pd.RangeIndex(100, name=\"x\"),\n)\nsource = source.reset_index().melt(\"x\", var_name=\"category\", value_name=\"y\")\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection_point(nearest=True, on=\"mouseover\", fields=[\"x\"], empty=False)\n\n# The basic line\nline = (\n    alt.Chart(source)\n    .mark_line(interpolate=\"basis\")\n    .encode(x=\"x:Q\", y=\"y:Q\", color=\"category:N\")\n)\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = (\n    alt.Chart(source)\n    .mark_point()\n    .encode(\n        x=\"x:Q\",\n        opacity=alt.value(0),\n    )\n    .add_params(nearest)\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align=\"left\", dx=5, dy=-5).encode(\n    text=alt.condition(nearest, \"y:Q\", alt.value(\" \"))\n)\n\n# Draw a rule at the location of the selection\nrules = (\n    alt.Chart(source)\n    .mark_rule(color=\"gray\")\n    .encode(\n        x=\"x:Q\",\n    )\n    .transform_filter(nearest)\n)\n\n# Put the five layers into a chart and bind the data\nalt.layer(line, selectors, points, rules, text).properties(\n    width=\"container\", height=300\n)\n\n\n\n\n\n\n\nWe can even embed a whole dataset and interact with it Tabelau style.\n\nimport pygwalker as pyg\n\npyg.walk(source)\n\n\n\n\n\n\n\nNotice you can drag and drop columns to change the graph to your heart‚Äôs content. A¬∑we¬∑some!"
  },
  {
    "objectID": "notebooks/pyscript.html",
    "href": "notebooks/pyscript.html",
    "title": "PyScript",
    "section": "",
    "text": "[splashscreen]\n        enabled = false\nSo‚Ä¶ this is the current date and time, computed by Python running in your browser!\nfrom datetime import datetime; now = datetime.now(); display(now.strftime(\"%m/%d/%Y, %H:%M:%S\"));"
  },
  {
    "objectID": "notebooks/pyscript.html#jupyter-repl",
    "href": "notebooks/pyscript.html#jupyter-repl",
    "title": "PyScript",
    "section": "Jupyter REPL",
    "text": "Jupyter REPL\nYou can also have an entire Jupyter REPL running in your browser, with the ability to run arbitrary Python code!\nGo ahead and try this out:\nfrom matplotlib import pyplot as plt\nimport numpy as np\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale) * 500)\nax.set(title=\"Some random data!\")\nplt.show()"
  },
  {
    "objectID": "notebooks/climate.html",
    "href": "notebooks/climate.html",
    "title": "Climate",
    "section": "",
    "text": "Evolution of the CO2 concentration in the atmosphere.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\ndf = pd.read_parquet(\"../data/climate_co2_global_trend.parquet\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Altair interactive line plot\nc = (\n    alt.Chart(df)\n    .mark_line()\n    .encode(\n        x=\"date:T\",\n        y=alt.Y(\"trend:Q\", scale=alt.Scale(domain=[390, 430]), title=\"CO2 (ppm)\"),\n        tooltip=[\"date:T\", \"trend:Q\"],\n    )\n    .properties(width=\"container\", height=200)\n    .configure_view(strokeWidth=0)\n    .configure_axis(grid=False, labelFontSize=14, titleFontSize=14)\n    .configure_title(fontSize=16, anchor=\"start\", color=\"gray\")\n    .display()\n)"
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "README.html#usage",
    "href": "README.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "README.html#what-can-you-do-with-datadex",
    "href": "README.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "README.html#setup",
    "href": "README.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "README.html#motivation",
    "href": "README.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "README.html#acknowledgements",
    "href": "README.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  },
  {
    "objectID": "notebooks/duckdb-ipfs.html",
    "href": "notebooks/duckdb-ipfs.html",
    "title": "Making DuckDB understand IPFS hashes",
    "section": "",
    "text": "Turns out you can query arbitrary filesystems with SQL. This notebook shows how to query IPFS with DuckDB.\nThe gist of it is that you can register any fsspec filesystem on DuckDB. As IPFS is supported by fsspec via ipfsspec, we can register it and query it with SQL.\nBefore running this code, you‚Äôll need to install ipfsspec. You can do so with:\npip install git+https://github.com/fsspec/ipfsspec\n\nimport duckdb\nfrom ipfsspec import AsyncIPFSFileSystem\n\nipfs_fs = AsyncIPFSFileSystem()\n\nduckdb.register_filesystem(ipfs_fs)\n\nOnce the filesystem is registered, you can use specific URIs inside read_csv_auto or read_parquet!\nA couple of examples:\n\n_ = (\n    duckdb.sql(\n        \"select * from read_csv_auto('ipfs://bafybeif5reawvqtsoybj5fhdl4ghaq3oc7kzepuws26zawkjm4johlv3uq')\"\n    )\n    .df()\n    .groupby(\"kingdom_name\")[\"taxonid\"]\n    .count()\n    .plot.bar()\n)\n\n\n\n\n\n_ = (\n    duckdb.query(\n        \"select * from read_parquet('ipfs://bafkreibnx5q6qwxobozkdm6xt7ktvwciyfvtkgy7fud67w5oyxnf5tch4e') limit 10\"\n    )\n    .df()\n    .groupby(\"year\")\n    .mean(\"literacy_rate\")\n    .plot()\n)"
  },
  {
    "objectID": "notebooks/brlda.html",
    "href": "notebooks/brlda.html",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "",
    "text": "Let‚Äôs start by loading the data.\nimport pandas as pd\n\nemployees_df = pd.read_csv(\"../data/employees.csv\")\nsafehouses_df = pd.read_csv(\"../data/safehouses.csv\")\ndivisions_df = pd.read_csv(\"../data/divisions.csv\")\nmanagers_df = pd.read_csv(\"../data/managers.csv\")\nactions_df = pd.read_csv(\"../data/actions.csv\")"
  },
  {
    "objectID": "notebooks/brlda.html#employees",
    "href": "notebooks/brlda.html#employees",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Employees",
    "text": "Employees\n\nemployees_df.sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n14007\n14010\nMichael Welch\nBusiness Analyst\nmichael_welch@brlda.gov\n614.574.6626\nGregory Wallace\n\n\n17813\n17817\nMelissa Delgado\nProject Manager\nmelissa_delgado@brlda.gov\n819-655-0418x56533\nSheila Mccormick\n\n\n15459\n15463\nMichael Reid\nBusiness Analyst\nmichael_reid@brlda.gov\n001-982-363-3908x964\nCharles Richards\n\n\n16765\n16769\nLawrence Davis\nBusiness Analyst\nlawrence_davis@brlda.gov\n838-728-0122\nMelinda Franklin\n\n\n4937\n4940\nLisa Cowan\nStatistician\nlisa_cowan@brlda.gov\n(210)491-5493x0294\nVictoria Willis\n\n\n\n\n\n\n\n\nemployees_df[\"EmployeeName\"].value_counts().head(5)\n\nEmployeeName\nMichael Smith       12\nDavid Smith         12\nLisa Smith          11\nJohn Smith          11\nMichael Williams    10\nName: count, dtype: int64\n\n\nSome duplicate employees are present in the data‚Ä¶\n\nemployees_df[employees_df[\"EmployeeName\"] == \"Michael Smith\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n4448\n4451\nMichael Smith\nProject Manager\nmichael_smith@brlda.gov\n001-137-479-2502x05426\nNathan Guerra\n\n\n5129\n5132\nMichael Smith\nScrum Master\nmichael_smith@brlda.gov\n223-274-6121x5268\nDavid Houston\n\n\n7455\n7458\nMichael Smith\nQuality Assurance Analyst\nmichael_smith@brlda.gov\n(202)737-6861x908\nPamela Hale\n\n\n10672\n10675\nMichael Smith\nData Scientist\nmichael_smith@brlda.gov\n001-192-379-3454x580\nSarah Brown\n\n\n11683\n11686\nMichael Smith\nQuality Assurance Analyst\nmichael_smith@brlda.gov\n001-038-692-4506x839\nPatrick Cruz\n\n\n13957\n13960\nMichael Smith\nProgram Manager\nmichael_smith@brlda.gov\n(471)896-2210x7208\nHeather Crawford\n\n\n15668\n15672\nMichael Smith\nScrum Master\nmichael_smith@brlda.gov\n668.014.9966x33380\nGregory Floyd\n\n\n17442\n17446\nMichael Smith\nData Analyst\nmichael_smith@brlda.gov\n\\t+1-544-294-4533x69316\nRichard Collins\n\n\n17570\n17574\nMichael Smith\nProgram Manager\nmichael_smith@brlda.gov\n9730334254\nMichele Graham\n\n\n19322\n19326\nMichael Smith\nMachine Learning Engineer\nmichael_smith@brlda.gov\n081-057-7739\nJoshua James\n\n\n22603\n22608\nMichael Smith\nProgram Manager\nmichael_smith@brlda.gov\n001-714-897-8059x3979\nAmber Garcia\n\n\n23884\n23889\nMichael Smith\nProject Manager\nmichael_smith@brlda.gov\n\\t+1-800-158-9222\nMichael Harris\n\n\n\n\n\n\n\nAre they all the same employee? The email match but the ID and Phone don‚Äôt.\nLet‚Äôs focus on the missing data. If they removed their users, we should see some missing EmployeeID values.\n\n# Check min and max EmployeeID\nmin_eid = employees_df[\"EmployeeID\"].min()\nmax_eid = employees_df[\"EmployeeID\"].max()\nprint(\"Min EmployeeID: \", min_eid)\nprint(\"Max EmployeeID: \", max_eid)\n\n# Check employees ids not in the min-max range\nmissing_employees = set(range(min_eid, max_eid + 1)) - set(employees_df[\"EmployeeID\"])\nprint(\"Employees ids not in the min-max range: \", missing_employees)\n\nMin EmployeeID:  1\nMax EmployeeID:  26849\nEmployees ids not in the min-max range:  {14976, 22602, 26188, 1423, 4284}\n\n\nThese might be the ‚Äúghosts‚Äù agents. Let‚Äôs quickly check that all managers are also employees just in case‚Ä¶\n\nmanagers = set(employees_df[\"Manager\"])\nemployees = set(employees_df[\"EmployeeName\"])\n\nmanagers - employees\n\nset()\n\n\nAre these ‚Äúghosts employees‚Äù in other tables?\n\ndivisions_df[divisions_df[\"EmployeeID\"].isin(missing_employees)]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n1422\n1423\nNaN\n[Division 7]\n[Project e-enable_holistic_models]\n[14, 214, 181, 219]\n\n\n4283\n4284\nNaN\n[Division 7]\n[Project repurpose_collaborative_methodologies...\n[10, 219]\n\n\n14975\n14976\nNaN\n[Division 7]\n[Project transform_24/365_functionalities]\n[25, 154, 231, 33, 219]\n\n\n22601\n22602\nNaN\n[Division 7]\n[Project monetize_one-to-one_mindshare, Projec...\n[12, 221, 19, 18, 219]\n\n\n26187\n26188\nNaN\n[Division 7]\n[Project extend_robust_action-items]\n[7, 219]\n\n\n\n\n\n\n\nThey all belong to Division 7. What actions did they take?\n\nactions_df[actions_df[\"EmployeeID\"].isin(missing_employees)].sort_values([\"ActionDate\"])\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\n\n\n\n\n41824\n14976\nQuantum Key Generation\n1994-06-06 00:00:00\nperform data mining on social media data for s...\nPuerto Rico\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n53036\n26188\nPredictive Modeling\n1994-11-20 00:00:00\nconstruct algorithms for automatic gait recogn...\nPuerto Rico\nfailed\ncritical\nProject extend_robust_action-items\nDivision 10\n\n\n4283\n4284\nData Clustering\n1996-05-08 00:00:00\nInitiate operation Networked_discrete_system_e...\nMartinique\ncompleted\nhigh\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n81969\n1423\nUser Profiling\n1997-04-12 00:00:00\nOperation Re-contextualized_attitude-oriented_...\nEgypt\nfailed\nmedium\nProject e-enable_holistic_models\nDivision 3\n\n\n68673\n14976\nNatural Language Generation\n2007-06-06 00:00:00\nconstruct algorithms for automatic vein recogn...\nBenin\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n31132\n4284\nQuantum Resistant Cryptography\n2007-09-17 00:00:00\nInitiate operation Customizable_discrete_paral...\nKazakhstan\nfailed\ncritical\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n57981\n4284\nMachine Learning-based Intrusion Detection\n2007-10-25 00:00:00\nanalyze communication patterns through Fully-c...\nAlbania\ncompleted\nhigh\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n28271\n1423\nAutomated Surveillance\n2009-10-18 00:00:00\nOperation Down-sized_24/7_capability to develo...\nPuerto Rico\nfailed\nhigh\nProject e-enable_holistic_models\nDivision 3\n\n\n14975\n14976\nNatural Language Generation\n2011-08-06 00:00:00\nInitiate operation Centralized_upward-trending...\nBahrain\ncompleted\nlow\nProject transform_24/365_functionalities\nDivision 1\n\n\n49450\n22602\nQuantum Key Generation\n2012-10-22 00:00:00\nOperation Digitized_methodical_structure to ap...\nIreland\nfailed\nhigh\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n55120\n1423\nData Leakage Detection\n2014-05-29 00:00:00\nInitiate operation Sharable_impactful_core, ta...\nAntarctica (the territory South of 60 deg S)\ncompleted\nlow\nProject e-enable_holistic_models\nDivision 3\n\n\n76299\n22602\nCryptocurrency Tracing\n2014-08-08 00:00:00\nanalyze network topology for vulnerabilities t...\nSan Marino\ncompleted\nmedium\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n95522\n14976\nPattern-based Malware Detection\n2015-12-12 00:00:00\nutilize machine learning for anomaly detection...\nCzech Republic\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n26187\n26188\nVoice Recognition\n2017-10-19 00:00:00\nperform text mining on classified documents th...\nBangladesh\nfailed\nmedium\nProject extend_robust_action-items\nDivision 10\n\n\n1422\n1423\nVideo Analysis\n2018-03-01 00:00:00\nInitiate operation Digitized_dedicated_help-de...\nUruguay\nfailed\nlow\nProject e-enable_holistic_models\nDivision 3\n\n\n22601\n22602\nCovert Behavioral Analytics\n2019-01-30 00:00:00\nOperation Vision-oriented_explicit_flexibility...\nIreland\nfailed\nmedium\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n79885\n26188\nData Extraction\n2020-12-01 00:00:00\nInitiate operation Realigned_exuding_Graphic_I...\nUnited States Minor Outlying Islands\nin progress\nlow\nProject extend_robust_action-items\nDivision 10\n\n\n84830\n4284\nCovert Social Network Analysis\n2021-03-30 00:00:00\nInitiate operation Self-enabling_tangible_webs...\nMexico\ncompleted\nlow\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n\n\n\n\n\nSome interesting things: - They all mention the devices in their action description. - They belong to a different Associated Division and not Division 7.\nHow common is for employees to do actions in other divisions? Time for OBT (one big table).\n\ndetailed_actions_df = actions_df.merge(\n    employees_df, left_on=\"EmployeeID\", right_on=\"EmployeeID\", how=\"left\"\n).merge(divisions_df, left_on=\"EmployeeID\", right_on=\"EmployeeID\", how=\"left\")\ndetailed_actions_df.sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\nEmployeeName_x\nJobTitle\nEmail\nPhone\nManager\nEmployeeName_y\nDivision\nProject\nknown_safehouses\n\n\n\n\n15083\n15084\nQuantum Key Generation\n2016-11-29 00:00:00\nInitiate operation Implemented_modular_attitud...\nPhilippines\ncompleted\nlow\nProject streamline_proactive_e-markets\nDivision 3\nJessica Jackson\nStatistician\njessica_jackson@brlda.gov\n\\t+1-852-619-7576\nPamela Nelson\nJessica Jackson\n[Division 9, Division 3]\n[Project unleash_front-end_models, Project str...\n[4, 232, 110, 220]\n\n\n21036\n21037\nNetwork Covert Channel Analysis\n2022-02-01 00:00:00\nOperation Profit-focused_6thgeneration_install...\nWestern Sahara\nin progress\nhigh\nProject streamline_proactive_e-markets\nDivision 4\nLisa Mccarty\nData Analyst\nlisa_mccarty@brlda.gov\n961.555.0296\nDarius Davis\nLisa Mccarty\n[Division 4]\n[Project streamline_proactive_e-markets]\n[31, 33, 155, 152]\n\n\n21892\n21893\nQuantitative Credit Scoring\n2019-02-14 00:00:00\nInitiate operation Open-source_solution-orient...\nCambodia\ncompleted\ncritical\nProject deliver_visionary_web-readiness\nDivision 4\nTerry Clark\nProject Manager\nterry_clark@brlda.gov\n001-856-450-8917x357\nCrystal Romero\nTerry Clark\n[Division 3, Division 4, Division 8]\n[Project monetize_one-to-one_mindshare, Projec...\n[20, 36, 20]\n\n\n65991\n12294\nDigital Forensics\n2019-05-15 00:00:00\ndevelop algorithms for automatic speech recogn...\nParaguay\ncompleted\nhigh\nProject mesh_cutting-edge_experiences\nDivision 4\nChris Smith\nProgram Manager\nchris_smith@brlda.gov\n001-606-260-6378x35048\nBryan Moore\nChris Smith\n[Division 4]\n[Project monetize_one-to-one_mindshare, Projec...\n[63]\n\n\n13255\n13256\nCovert Biometric Identification\n2002-08-02 00:00:00\nconstruct algorithms for intrusion detection i...\nEl Salvador\ncompleted\ncritical\nProject facilitate_mission-critical_ROI\nDivision 9\nTimothy Johnson\nProgram Manager\ntimothy_johnson@brlda.gov\n887-799-0591\nJames Rogers\nTimothy Johnson\n[Division 9, Division 10]\n[Project facilitate_mission-critical_ROI, Proj...\n[13, 19, 1, 191]\n\n\n\n\n\n\n\n\ndef check_division_associated_division(row):\n    return row[\"AssociatedDivision\"] in row[\"Division\"]\n\n\ndetailed_actions_df[\n    ~detailed_actions_df.apply(check_division_associated_division, axis=1)\n].sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\nEmployeeName_x\nJobTitle\nEmail\nPhone\nManager\nEmployeeName_y\nDivision\nProject\nknown_safehouses\n\n\n\n\n35605\n8757\nPattern-based Malware Detection\n2001-10-17 00:00:00\nInitiate operation Adaptive_next_generation_le...\nTuvalu\nfailed\nlow\nProject repurpose_collaborative_methodologies\nDivision 7\nKimberly Mccann\nProject Manager\nkimberly_mccann@brlda.gov\n\\t+1-751-319-4440x1839\nChristopher Sanchez\nKimberly Mccann\n[Division 9, Division 2, Division 2]\n[Project repurpose_collaborative_methodologies]\n[77]\n\n\n77354\n23657\nQuantum Key Generation\n2021-12-01 00:00:00\nOperation Cross-group_system-worthy_function t...\nMarshall Islands\nfailed\nmedium\nProject facilitate_mission-critical_ROI\nDivision 7\nMichael Garza\nBusiness Analyst\nmichael_garza@brlda.gov\n001-960-646-5296\nBrandon Wiley Jr.\nMichael Garza\n[Division 10, Division 4, Division 6]\n[Project scale_back-end_interfaces, Project di...\n[111, 46, 149]\n\n\n23463\n23464\nMalware Analysis\n2008-07-20 00:00:00\nInitiate operation Expanded_24hour_firmware, t...\nBritish Virgin Islands\ncompleted\nhigh\nProject monetize_one-to-one_mindshare\nDivision 7\nMatthew Boyd\nProgram Manager\nmatthew_boyd@brlda.gov\n953-103-4209x1001\nCrystal Harris\nMatthew Boyd\n[Division 2, Division 2, Division 10]\n[Project monetize_one-to-one_mindshare]\n[19, 121, 152]\n\n\n31975\n5127\nData Steganalysis\n2018-03-08 00:00:00\nInitiate operation Synchronized_responsive_kno...\nGreece\nfailed\nhigh\nProject embrace_magnetic_systems\nDivision 7\nAudrey Peters\nBusiness Analyst\naudrey_peters@brlda.gov\n\\t+1-512-390-9160x5974\nShane Cowan\nAudrey Peters\n[Division 8, Division 9, Division 2]\n[Project drive_value-added_mindshare, Project ...\n[224, 193, 129]\n\n\n18674\n18675\nData Access Control\n2006-04-25 00:00:00\nOperation Pre-emptive_object-oriented_adapter ...\nSyrian Arab Republic\nfailed\ncritical\nProject engineer_killer_applications\nDivision 7\nJohn Willis\nScrum Master\njohn_willis@brlda.gov\n001-930-326-5306\nJames Frost\nJohn Willis\n[Division 10, Division 5, Division 9]\n[Project engineer_killer_applications]\n[21, 50, 110, 152]\n\n\n\n\n\n\n\nSome people did actions associated to Division 7 but is not reflected in the Employee table Divisions.\n\ndetailed_actions_df[\n    ~detailed_actions_df.apply(check_division_associated_division, axis=1)\n][\"AssociatedDivision\"].value_counts()\n\nAssociatedDivision\nDivision 7     2284\nDivision 6        7\nDivision 3        4\nDivision 1        4\nDivision 10       3\nName: count, dtype: int64\n\n\nNow, most of the people that did actions in Division 7 are missclassified, but there are some that did actions in the divisions 1, 3, 6, and 10. These are our suspects."
  },
  {
    "objectID": "notebooks/brlda.html#safehouses",
    "href": "notebooks/brlda.html#safehouses",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Safehouses",
    "text": "Safehouses\n\nsafehouses_df.sample(5)\n\n\n\n\n\n\n\n\nID\nCity\nAddress\nLatitude\nLongitude\n\n\n\n\n41\n158\nMoscow\nIzmaylovo Manor, –ü–µ—Ä–≤–æ–º–∞–π—Å–∫–∞—è —É–ª–∏—Ü–∞, Izmaylovo...\n55.794138\n37.757960\n\n\n169\n13\nMoscow\n–î–µ—Ç—Å–∫–∏–π –æ–∑–¥–æ—Ä–æ–≤–∏—Ç–µ–ª—å–Ω—ã–π –ª–∞–≥–µ—Ä—å \"–ò—Å–∫–æ—Ä–∫–∞\", –ú-2,...\n55.363000\n37.727959\n\n\n25\n192\nRome\nMaiorana, Via Bolognola, 63, 00138 Rome RM, Italy\n41.987777\n12.503599\n\n\n120\n43\nJakarta\nJalan Zeni AD I, Pancoran, Special Capital Reg...\n-6.260742\n106.856980\n\n\n2\n231\nLuang Prabang\n13, 10554 Khouathineung, Laos\n19.843087\n102.159075\n\n\n\n\n\n\n\n\n# Map of safehouses with Latitude and Longitude\nimport folium\n\nsafehouses_map = folium.Map(\n    location=[safehouses_df[\"Latitude\"].mean(), safehouses_df[\"Longitude\"].mean()],\n    zoom_start=4,\n)\n\nfor index, row in safehouses_df.iterrows():\n    folium.Marker([row[\"Latitude\"], row[\"Longitude\"]], popup=row[\"ID\"]).add_to(\n        safehouses_map\n    )\n\nsafehouses_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nCute, but not very useful for now, let‚Äôs explore divisions."
  },
  {
    "objectID": "notebooks/brlda.html#divisions",
    "href": "notebooks/brlda.html#divisions",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Divisions",
    "text": "Divisions\n\ndivisions_df\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n0\n1\nKelly Rios\n[Division 1]\n[Project deliver_visionary_web-readiness]\n[232, 1, 73, 217]\n\n\n1\n2\nMadison Barr\n[Division 6, Division 3]\n[Project repurpose_collaborative_methodologies]\n[192, 26, 118, 4]\n\n\n2\n3\nSue Anderson\n[Division 5, Division 1, Division 10]\n[Project repurpose_collaborative_methodologies]\n[19, 8, 130, 50]\n\n\n3\n4\nLaura Carlson\n[Division 9, Division 9, Division 2]\n[Project streamline_proactive_e-markets]\n[15, 232]\n\n\n4\n5\nCarrie Ali\n[Division 3, Division 6]\n[Project deliver_visionary_web-readiness]\n[158, 118]\n\n\n...\n...\n...\n...\n...\n...\n\n\n26844\n26845\nChristopher Riley\n[Division 8, Division 10, Division 5]\n[Project drive_value-added_mindshare, Project ...\n[226]\n\n\n26845\n26846\nEric Chan\n[Division 2, Division 6, Division 1]\n[Project deliver_visionary_web-readiness]\n[15, 99, 58]\n\n\n26846\n26847\nAmy Vazquez\n[Division 8, Division 10, Division 5]\n[Project embrace_transparent_networks, Project...\n[22]\n\n\n26847\n26848\nClifford Reyes\n[Division 9, Division 2, Division 6]\n[Project disintermediate_distributed_experienc...\n[98]\n\n\n26848\n26849\nNoah Snyder\n[Division 2, Division 9, Division 10]\n[Project strategize_value-added_bandwidth]\n[1]\n\n\n\n\n26849 rows √ó 5 columns\n\n\n\n\nManagers\n\nmanagers_df.sample(5)\n\n\n\n\n\n\n\n\nManagerName\nEmployee_1\nEmployee_2\nEmployee_3\nEmployee_4\nEmployee_5\nEmployee_6\nEmployee_7\nEmployee_8\nEmployee_9\n...\nEmployee_20\nEmployee_21\nEmployee_22\nEmployee_23\nEmployee_24\nEmployee_25\nEmployee_26\nEmployee_27\nEmployee_28\nEmployee_29\n\n\n\n\n601\nMichael Payne DDS\nSusan Hanson\nHeather Brown\nMegan Mayo\nRachel Nguyen\nNancy West\nJennifer Rodriguez\nCarol Norton\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2542\nMark Marshall\nChristopher Lang\nDawn Diaz\nTeresa Woods\nAnthony Garrett\nRobert Hodges\nHolly Kim\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2413\nSharon Salazar\nJacob Stanley\nBrooke Perry\nLori Riddle\nKelly Lin\nJennifer Matthews\nAnthony West\nJason Hawkins\nKimberly Wood\nJared Zimmerman\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3246\nAmanda Daniels\nAbigail Acosta\nConnie Davis\nChristina Vang\nCynthia Morris\nLauren Walsh\nClaire Chandler\nJoseph Nelson\nTiffany Mckee\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3445\nRenee Gonzales\nCaroline Rogers\nAdam Sanchez\nMr. James Melton\nTanner Villa\nCandice Mann\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 30 columns\n\n\n\n\n# Transform to long format\nclean_managers_df = (\n    managers_df.melt(\n        id_vars=[\"ManagerName\"], value_name=\"EmployeeName\", var_name=\"EmployeeNumber\"\n    )\n    .dropna()\n    .drop(columns=[\"EmployeeNumber\"])\n)\n\nclean_managers_df.sample(5)\n\n\n\n\n\n\n\n\nManagerName\nEmployeeName\n\n\n\n\n17713\nAndrew Collins\nAmber Myers\n\n\n10222\nTodd Ayala\nAntonio Steele\n\n\n26918\nBrandon Wiley Jr.\nHerbert Moore\n\n\n6747\nTodd Wilson\nKenneth Reed\n\n\n24041\nLisa Bowman\nTracy Burns\n\n\n\n\n\n\n\n\nmanagers_df[managers_df[\"Employee_1\"].isna()]\n\n\n\n\n\n\n\n\nManagerName\nEmployee_1\nEmployee_2\nEmployee_3\nEmployee_4\nEmployee_5\nEmployee_6\nEmployee_7\nEmployee_8\nEmployee_9\n...\nEmployee_20\nEmployee_21\nEmployee_22\nEmployee_23\nEmployee_24\nEmployee_25\nEmployee_26\nEmployee_27\nEmployee_28\nEmployee_29\n\n\n\n\n3686\nJessica Stone\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1 rows √ó 30 columns\n\n\n\nJessica Stone is the only manager without any employees (unless there are NaNs gaps).\n\nclean_managers_df[clean_managers_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nManagerName\nEmployeeName\n\n\n\n\n17210\nChristopher Mckenzie\nJessica Stone\n\n\n\n\n\n\n\n\nemployees_df[employees_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n11560\n11563\nJessica Stone\nData Scientist\njessica_stone@brlda.gov\n001-234-563-9331\nChristopher Mckenzie\n\n\n\n\n\n\n\n\ndivisions_df[divisions_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n11562\n11563\nJessica Stone\n[Division 4]\n[Project mesh_cutting-edge_experiences, Projec...\n[18, 42, 44]\n\n\n\n\n\n\n\n\nactions_df[actions_df[\"EmployeeID\"] == 11563]\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\n\n\n\n\n11562\n11563\nGesture Recognition\n2016-06-21 00:00:00\nbuild systems for automatic object detection i...\nSyrian Arab Republic\nfailed\ncritical\nProject unleash_front-end_models\nDivision 4\n\n\n38411\n11563\nAutomated Social Media Profiling\n2012-08-31 00:00:00\nOperation Optimized_real-time_artificial_intel...\nBotswana\nfailed\nlow\nProject unleash_front-end_models\nDivision 4\n\n\n65260\n11563\nObject Recognition\n1996-05-13 00:00:00\nperform data mining on financial transactions ...\nCongo\ncompleted\nmedium\nProject unleash_front-end_models\nDivision 4\n\n\n92109\n11563\nCovert Facial Recognition\n1999-05-26 00:00:00\nInitiate operation Visionary_coherent_architec...\nItaly\ncompleted\ncritical\nProject unleash_front-end_models\nDivision 4"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "index.html#what-can-you-do-with-datadex",
    "href": "index.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  }
]