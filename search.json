[
  {
    "objectID": "notebooks/climate.html",
    "href": "notebooks/climate.html",
    "title": "Climate",
    "section": "",
    "text": "Evolution of the CO2 concentration in the atmosphere.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\ndf = pd.read_parquet(\"../data/climate_co2_global_trend.parquet\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Altair interactive line plot\n\nc = (\n    alt.Chart(df)\n    .mark_line()\n    .encode(\n        x=\"date:T\",\n        y=alt.Y(\"trend:Q\", scale=alt.Scale(domain=[390, 430]), title=\"CO2 (ppm)\"),\n        tooltip=[\"date:T\", \"trend:Q\"],\n    )\n    .properties(width=\"container\", height=200)\n    .configure_view(strokeWidth=0)\n    .configure_axis(grid=False, labelFontSize=14, titleFontSize=14)\n    .configure_title(fontSize=16, anchor=\"start\", color=\"gray\")\n    .display()\n)"
  },
  {
    "objectID": "notebooks/duckdb-ipfs.html",
    "href": "notebooks/duckdb-ipfs.html",
    "title": "DuckDB and IPFS",
    "section": "",
    "text": "Turns out you can query arbitrary filesystems with SQL. This notebook shows how to query IPFS with DuckDB.\nThe gist of it is that you can register any fsspec filesystem on DuckDB. As IPFS is supported by fsspec via ipfsspec, we can register it and query it with SQL.\nBefore running this code, you‚Äôll need to install ipfsspec. You can do so with:\npip install git+https://github.com/fsspec/ipfsspec\n\n\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport duckdb\nfrom ipfsspec import AsyncIPFSFileSystem\n\nipfs_fs = AsyncIPFSFileSystem()\n\nduckdb.register_filesystem(ipfs_fs)\n\nOnce the filesystem is registered, you can use specific URIs inside read_csv_auto or read_parquet!\nA couple of examples:\n\n_ = (\n    duckdb.sql(\n        \"select * from read_csv_auto('ipfs://bafybeif5reawvqtsoybj5fhdl4ghaq3oc7kzepuws26zawkjm4johlv3uq')\"\n    )\n    .df()\n    .groupby(\"kingdom_name\")[\"taxonid\"]\n    .count()\n    .plot.bar()\n)\n\n\n\n\n\n_ = (\n    duckdb.query(\n        \"select * from read_parquet('ipfs://bafkreibnx5q6qwxobozkdm6xt7ktvwciyfvtkgy7fud67w5oyxnf5tch4e') limit 10\"\n    )\n    .df()\n    .groupby(\"year\")\n    .mean(\"literacy_rate\")\n    .plot()\n)"
  },
  {
    "objectID": "notebooks/quarto.html",
    "href": "notebooks/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "This page contains a few examples of how to do things in Quarto‚Äôs world.\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ntable = [\n    [\"Sun\", 696000, 1989100000],\n    [\"Earth\", 6371, 5973.6],\n    [\"Moon\", 1737, 73.5],\n    [\"Mars\", 3390, 641.85],\n]\nMarkdown(tabulate(table, headers=[\"Planet\", \"R (km)\", \"mass (x 10^29 kg)\"]))\n\n\n\nTable¬†1: Planets\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 23, 2, 4])\nplt.show()\n\nplt.plot([8, 65, 23, 90])\nplt.show()\n\n\n\n\n\n\n\n(a) First\n\n\n\n\n\n\n\n(b) Second\n\n\n\n\nFigure¬†1: Charts"
  },
  {
    "objectID": "notebooks/quarto.html#intro",
    "href": "notebooks/quarto.html#intro",
    "title": "Quarto",
    "section": "",
    "text": "This page contains a few examples of how to do things in Quarto‚Äôs world.\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ntable = [\n    [\"Sun\", 696000, 1989100000],\n    [\"Earth\", 6371, 5973.6],\n    [\"Moon\", 1737, 73.5],\n    [\"Mars\", 3390, 641.85],\n]\nMarkdown(tabulate(table, headers=[\"Planet\", \"R (km)\", \"mass (x 10^29 kg)\"]))\n\n\n\nTable¬†1: Planets\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 23, 2, 4])\nplt.show()\n\nplt.plot([8, 65, 23, 90])\nplt.show()\n\n\n\n\n\n\n\n(a) First\n\n\n\n\n\n\n\n(b) Second\n\n\n\n\nFigure¬†1: Charts"
  },
  {
    "objectID": "notebooks/quarto.html#dynamic-graphs",
    "href": "notebooks/quarto.html#dynamic-graphs",
    "title": "Quarto",
    "section": "Dynamic Graphs",
    "text": "Dynamic Graphs\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nsource = pd.DataFrame(\n    np.cumsum(np.random.randn(100, 3), 0).round(2),\n    columns=[\"A\", \"B\", \"C\"],\n    index=pd.RangeIndex(100, name=\"x\"),\n)\nsource = source.reset_index().melt(\"x\", var_name=\"category\", value_name=\"y\")\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection_point(nearest=True, on=\"mouseover\", fields=[\"x\"], empty=False)\n\n# The basic line\nline = (\n    alt.Chart(source)\n    .mark_line(interpolate=\"basis\")\n    .encode(x=\"x:Q\", y=\"y:Q\", color=\"category:N\")\n)\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = (\n    alt.Chart(source)\n    .mark_point()\n    .encode(\n        x=\"x:Q\",\n        opacity=alt.value(0),\n    )\n    .add_params(nearest)\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align=\"left\", dx=5, dy=-5).encode(\n    text=alt.condition(nearest, \"y:Q\", alt.value(\" \"))\n)\n\n# Draw a rule at the location of the selection\nrules = (\n    alt.Chart(source)\n    .mark_rule(color=\"gray\")\n    .encode(\n        x=\"x:Q\",\n    )\n    .transform_filter(nearest)\n)\n\n# Put the five layers into a chart and bind the data\nalt.layer(line, selectors, points, rules, text).properties(\n    width=\"container\", height=300\n)\n\n\n\n\n\n\n\nYou can even embed a whole dataset and interact with it Tabelau style.\n\nimport pygwalker as pyg\n\npyg.walk(source)"
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "README.html#usage",
    "href": "README.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "README.html#what-can-you-do-with-datadex",
    "href": "README.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "README.html#setup",
    "href": "README.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "README.html#motivation",
    "href": "README.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "README.html#acknowledgements",
    "href": "README.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  },
  {
    "objectID": "notebooks/pyscript.html",
    "href": "notebooks/pyscript.html",
    "title": "PyScript",
    "section": "",
    "text": "[splashscreen]\n        enabled = false\nSo‚Ä¶ this is the current date and time, computed by Python running in your browser!\nfrom datetime import datetime; now = datetime.now(); display(now.strftime(\"%m/%d/%Y, %H:%M:%S\"));"
  },
  {
    "objectID": "notebooks/pyscript.html#jupyter-repl",
    "href": "notebooks/pyscript.html#jupyter-repl",
    "title": "PyScript",
    "section": "Jupyter REPL",
    "text": "Jupyter REPL\nYou can also have an entire Jupyter REPL running in your browser, with the ability to run arbitrary Python code!\nGo ahead and try this out:\nfrom matplotlib import pyplot as plt\nimport numpy as np\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale) * 500)\nax.set(title=\"Some random data!\")\nplt.show()"
  },
  {
    "objectID": "notebooks/brlda.html",
    "href": "notebooks/brlda.html",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "",
    "text": "Let‚Äôs start by loading the data and taking a look at it.\n\nimport pandas as pd\n\nemployees_df = pd.read_csv(\"../data/employees.csv\")\nsafehouses_df = pd.read_csv(\"../data/safehouses.csv\")\ndivisions_df = pd.read_csv(\"../data/divisions.csv\")\nmanagers_df = pd.read_csv(\"../data/managers.csv\")\nactions_df = pd.read_csv(\"../data/actions.csv\")\n\nExamining Employees\n\nemployees_df.sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n10786\n10789\nPamela Doyle\nMachine Learning Engineer\npamela_doyle@brlda.gov\n3391516360\nLaura Stevens\n\n\n5238\n5241\nJulie Rogers\nData Analyst\njulie_rogers@brlda.gov\n(687)006-0887x908\nJames Rodriguez\n\n\n18701\n18705\nMatthew Winters\nData Analyst\nmatthew_winters@brlda.gov\n231.857.6562\nMichelle Kramer\n\n\n9185\n9188\nJessica Singh\nProject Manager\njessica_singh@brlda.gov\n(658)504-4366x7007\nLeslie Larson\n\n\n12780\n12783\nLeslie Moody\nData Analyst\nleslie_moody@brlda.gov\n566-283-2550x452\nGina Johnson\n\n\n\n\n\n\n\nIf they removed their users, we should see some missing EmployeeID values.\n\n# Check min and max EmployeeID\nmin_eid = employees_df[\"EmployeeID\"].min()\nmax_eid = employees_df[\"EmployeeID\"].max()\nprint(\"Min EmployeeID: \", min_eid)\nprint(\"Max EmployeeID: \", max_eid)\n\n# Check employees ids not in the min-max range\nmissing_employees = set(range(min_eid, max_eid + 1)) - set(employees_df[\"EmployeeID\"])\nprint(\"Employees ids not in the min-max range: \", missing_employees)\n\nMin EmployeeID:  1\nMax EmployeeID:  26849\nEmployees ids not in the min-max range:  {14976, 22602, 26188, 1423, 4284}\n\n\nAre they in other tables?\n\ndivisions_df[divisions_df[\"EmployeeID\"].isin(missing_employees)]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n1422\n1423\nNaN\n[Division 7]\n[Project e-enable_holistic_models]\n[14, 214, 181, 219]\n\n\n4283\n4284\nNaN\n[Division 7]\n[Project repurpose_collaborative_methodologies...\n[10, 219]\n\n\n14975\n14976\nNaN\n[Division 7]\n[Project transform_24/365_functionalities]\n[25, 154, 231, 33, 219]\n\n\n22601\n22602\nNaN\n[Division 7]\n[Project monetize_one-to-one_mindshare, Projec...\n[12, 221, 19, 18, 219]\n\n\n26187\n26188\nNaN\n[Division 7]\n[Project extend_robust_action-items]\n[7, 219]\n\n\n\n\n\n\n\n\nactions_df[actions_df[\"EmployeeID\"].isin(missing_employees)].sort_values(\"ActionDate\")\n\n\n\n\n\n\n\n\nEmployeeID\nActionType\nActionDate\nActionDescription\nActionLocation\nActionStatus\nActionSeverity\nAssociatedProject\nAssociatedDivision\n\n\n\n\n41824\n14976\nQuantum Key Generation\n1994-06-06 00:00:00\nperform data mining on social media data for s...\nPuerto Rico\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n53036\n26188\nPredictive Modeling\n1994-11-20 00:00:00\nconstruct algorithms for automatic gait recogn...\nPuerto Rico\nfailed\ncritical\nProject extend_robust_action-items\nDivision 10\n\n\n4283\n4284\nData Clustering\n1996-05-08 00:00:00\nInitiate operation Networked_discrete_system_e...\nMartinique\ncompleted\nhigh\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n81969\n1423\nUser Profiling\n1997-04-12 00:00:00\nOperation Re-contextualized_attitude-oriented_...\nEgypt\nfailed\nmedium\nProject e-enable_holistic_models\nDivision 3\n\n\n68673\n14976\nNatural Language Generation\n2007-06-06 00:00:00\nconstruct algorithms for automatic vein recogn...\nBenin\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n31132\n4284\nQuantum Resistant Cryptography\n2007-09-17 00:00:00\nInitiate operation Customizable_discrete_paral...\nKazakhstan\nfailed\ncritical\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n57981\n4284\nMachine Learning-based Intrusion Detection\n2007-10-25 00:00:00\nanalyze communication patterns through Fully-c...\nAlbania\ncompleted\nhigh\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n28271\n1423\nAutomated Surveillance\n2009-10-18 00:00:00\nOperation Down-sized_24/7_capability to develo...\nPuerto Rico\nfailed\nhigh\nProject e-enable_holistic_models\nDivision 3\n\n\n14975\n14976\nNatural Language Generation\n2011-08-06 00:00:00\nInitiate operation Centralized_upward-trending...\nBahrain\ncompleted\nlow\nProject transform_24/365_functionalities\nDivision 1\n\n\n49450\n22602\nQuantum Key Generation\n2012-10-22 00:00:00\nOperation Digitized_methodical_structure to ap...\nIreland\nfailed\nhigh\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n55120\n1423\nData Leakage Detection\n2014-05-29 00:00:00\nInitiate operation Sharable_impactful_core, ta...\nAntarctica (the territory South of 60 deg S)\ncompleted\nlow\nProject e-enable_holistic_models\nDivision 3\n\n\n76299\n22602\nCryptocurrency Tracing\n2014-08-08 00:00:00\nanalyze network topology for vulnerabilities t...\nSan Marino\ncompleted\nmedium\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n95522\n14976\nPattern-based Malware Detection\n2015-12-12 00:00:00\nutilize machine learning for anomaly detection...\nCzech Republic\ncompleted\ncritical\nProject transform_24/365_functionalities\nDivision 1\n\n\n26187\n26188\nVoice Recognition\n2017-10-19 00:00:00\nperform text mining on classified documents th...\nBangladesh\nfailed\nmedium\nProject extend_robust_action-items\nDivision 10\n\n\n1422\n1423\nVideo Analysis\n2018-03-01 00:00:00\nInitiate operation Digitized_dedicated_help-de...\nUruguay\nfailed\nlow\nProject e-enable_holistic_models\nDivision 3\n\n\n22601\n22602\nCovert Behavioral Analytics\n2019-01-30 00:00:00\nOperation Vision-oriented_explicit_flexibility...\nIreland\nfailed\nmedium\nProject monetize_one-to-one_mindshare\nDivision 6\n\n\n79885\n26188\nData Extraction\n2020-12-01 00:00:00\nInitiate operation Realigned_exuding_Graphic_I...\nUnited States Minor Outlying Islands\nin progress\nlow\nProject extend_robust_action-items\nDivision 10\n\n\n84830\n4284\nCovert Social Network Analysis\n2021-03-30 00:00:00\nInitiate operation Self-enabling_tangible_webs...\nMexico\ncompleted\nlow\nProject repurpose_collaborative_methodologies\nDivision 6\n\n\n\n\n\n\n\nLet‚Äôs take a look at Safehouses now.\n\nsafehouses_df.sample(5)\n\n\n\n\n\n\n\n\nID\nCity\nAddress\nLatitude\nLongitude\n\n\n\n\n135\n31\nMexico City\n52660 Rancho Jim√©nez, MEX, Mexico\n19.128836\n-99.371064\n\n\n47\n152\nParis\nRue de Montubois, 95840 B√©themont-la-For√™t, Fr...\n49.049567\n2.241954\n\n\n167\n14\nMoscow\n46–ù-05815, –ü—Ä–æ—Ç–∞—Å–æ–≤–æ, Moscow Oblast, Russia, 1...\n56.136233\n37.619822\n\n\n60\n130\nDavao City\nAngalan Road, Davao City, 8022 Davao Region, P...\n7.118768\n125.461715\n\n\n40\n159\nMoscow\n–ò—Ä–ª–∞–Ω–¥—Å–∫–∞—è —É–ª–∏—Ü–∞ 33, –ú–∏–Ω–∑–∞–≥, Moscow, Russia, 1...\n55.440171\n37.326182\n\n\n\n\n\n\n\n\n# Map of safehouses with Latitude and Longitude\nimport folium\n\nsafehouses_map = folium.Map(\n    location=[safehouses_df[\"Latitude\"].mean(), safehouses_df[\"Longitude\"].mean()],\n    zoom_start=12,\n)\n\nfor index, row in safehouses_df.iterrows():\n    folium.Marker([row[\"Latitude\"], row[\"Longitude\"]], popup=row[\"ID\"]).add_to(\n        safehouses_map\n    )\n\nsafehouses_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "index.html#what-can-you-do-with-datadex",
    "href": "index.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  }
]