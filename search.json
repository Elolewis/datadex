[
  {
    "objectID": "notebooks/climate.html",
    "href": "notebooks/climate.html",
    "title": "Climate",
    "section": "",
    "text": "Evolution of the CO2 concentration in the atmosphere.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\ndf = pd.read_parquet(\"../data/climate_co2_global_trend.parquet\")\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Altair interactive line plot\n\nc = (\n    alt.Chart(df)\n    .mark_line()\n    .encode(\n        x=\"date:T\",\n        y=alt.Y(\"trend:Q\", scale=alt.Scale(domain=[390, 430]), title=\"CO2 (ppm)\"),\n        tooltip=[\"date:T\", \"trend:Q\"],\n    )\n    .properties(width=\"container\", height=200)\n    .configure_view(strokeWidth=0)\n    .configure_axis(grid=False, labelFontSize=14, titleFontSize=14)\n    .configure_title(fontSize=16, anchor=\"start\", color=\"gray\")\n    .display()\n)"
  },
  {
    "objectID": "notebooks/duckdb-ipfs.html",
    "href": "notebooks/duckdb-ipfs.html",
    "title": "DuckDB and IPFS",
    "section": "",
    "text": "Turns out you can query arbitrary filesystems with SQL. This notebook shows how to query IPFS with DuckDB.\nThe gist of it is that you can register any fsspec filesystem on DuckDB. As IPFS is supported by fsspec via ipfsspec, we can register it and query it with SQL.\nBefore running this code, you‚Äôll need to install ipfsspec. You can do so with:\npip install git+https://github.com/fsspec/ipfsspec\n\n\n\n[notice] A new release of pip available: 22.3.1 -&gt; 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport duckdb\nfrom ipfsspec import AsyncIPFSFileSystem\n\nipfs_fs = AsyncIPFSFileSystem()\n\nduckdb.register_filesystem(ipfs_fs)\n\nOnce the filesystem is registered, you can use specific URIs inside read_csv_auto or read_parquet!\nA couple of examples:\n\n_ = (\n    duckdb.sql(\n        \"select * from read_csv_auto('ipfs://bafybeif5reawvqtsoybj5fhdl4ghaq3oc7kzepuws26zawkjm4johlv3uq')\"\n    )\n    .df()\n    .groupby(\"kingdom_name\")[\"taxonid\"]\n    .count()\n    .plot.bar()\n)\n\n\n\n\n\n_ = (\n    duckdb.query(\n        \"select * from read_parquet('ipfs://bafkreibnx5q6qwxobozkdm6xt7ktvwciyfvtkgy7fud67w5oyxnf5tch4e') limit 10\"\n    )\n    .df()\n    .groupby(\"year\")\n    .mean(\"literacy_rate\")\n    .plot()\n)"
  },
  {
    "objectID": "notebooks/quarto.html",
    "href": "notebooks/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "This page contains a few examples of how to do things in Quarto‚Äôs world.\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ntable = [\n    [\"Sun\", 696000, 1989100000],\n    [\"Earth\", 6371, 5973.6],\n    [\"Moon\", 1737, 73.5],\n    [\"Mars\", 3390, 641.85],\n]\nMarkdown(tabulate(table, headers=[\"Planet\", \"R (km)\", \"mass (x 10^29 kg)\"]))\n\n\n\nTable¬†1: Planets\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 23, 2, 4])\nplt.show()\n\nplt.plot([8, 65, 23, 90])\nplt.show()\n\n\n\n\n\n\n\n(a) First\n\n\n\n\n\n\n\n(b) Second\n\n\n\n\nFigure¬†1: Charts"
  },
  {
    "objectID": "notebooks/quarto.html#intro",
    "href": "notebooks/quarto.html#intro",
    "title": "Quarto",
    "section": "",
    "text": "This page contains a few examples of how to do things in Quarto‚Äôs world.\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ntable = [\n    [\"Sun\", 696000, 1989100000],\n    [\"Earth\", 6371, 5973.6],\n    [\"Moon\", 1737, 73.5],\n    [\"Mars\", 3390, 641.85],\n]\nMarkdown(tabulate(table, headers=[\"Planet\", \"R (km)\", \"mass (x 10^29 kg)\"]))\n\n\n\nTable¬†1: Planets\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 23, 2, 4])\nplt.show()\n\nplt.plot([8, 65, 23, 90])\nplt.show()\n\n\n\n\n\n\n\n(a) First\n\n\n\n\n\n\n\n(b) Second\n\n\n\n\nFigure¬†1: Charts"
  },
  {
    "objectID": "notebooks/quarto.html#dynamic-graphs",
    "href": "notebooks/quarto.html#dynamic-graphs",
    "title": "Quarto",
    "section": "Dynamic Graphs",
    "text": "Dynamic Graphs\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nsource = pd.DataFrame(\n    np.cumsum(np.random.randn(100, 3), 0).round(2),\n    columns=[\"A\", \"B\", \"C\"],\n    index=pd.RangeIndex(100, name=\"x\"),\n)\nsource = source.reset_index().melt(\"x\", var_name=\"category\", value_name=\"y\")\n\n# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection_point(nearest=True, on=\"mouseover\", fields=[\"x\"], empty=False)\n\n# The basic line\nline = (\n    alt.Chart(source)\n    .mark_line(interpolate=\"basis\")\n    .encode(x=\"x:Q\", y=\"y:Q\", color=\"category:N\")\n)\n\n# Transparent selectors across the chart. This is what tells us\n# the x-value of the cursor\nselectors = (\n    alt.Chart(source)\n    .mark_point()\n    .encode(\n        x=\"x:Q\",\n        opacity=alt.value(0),\n    )\n    .add_params(nearest)\n)\n\n# Draw points on the line, and highlight based on selection\npoints = line.mark_point().encode(\n    opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n)\n\n# Draw text labels near the points, and highlight based on selection\ntext = line.mark_text(align=\"left\", dx=5, dy=-5).encode(\n    text=alt.condition(nearest, \"y:Q\", alt.value(\" \"))\n)\n\n# Draw a rule at the location of the selection\nrules = (\n    alt.Chart(source)\n    .mark_rule(color=\"gray\")\n    .encode(\n        x=\"x:Q\",\n    )\n    .transform_filter(nearest)\n)\n\n# Put the five layers into a chart and bind the data\nalt.layer(line, selectors, points, rules, text).properties(\n    width=\"container\", height=300\n)\n\n\n\n\n\n\n\nYou can even embed a whole dataset and interact with it Tabelau style.\n\nimport pygwalker as pyg\n\npyg.walk(source)"
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "README.html#usage",
    "href": "README.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "README.html#what-can-you-do-with-datadex",
    "href": "README.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "README.html#setup",
    "href": "README.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "README.html#motivation",
    "href": "README.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "README.html#acknowledgements",
    "href": "README.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  },
  {
    "objectID": "notebooks/pyscript.html",
    "href": "notebooks/pyscript.html",
    "title": "PyScript",
    "section": "",
    "text": "[splashscreen]\n        enabled = false\nSo‚Ä¶ this is the current date and time, computed by Python running in your browser!\nfrom datetime import datetime; now = datetime.now(); display(now.strftime(\"%m/%d/%Y, %H:%M:%S\"));"
  },
  {
    "objectID": "notebooks/pyscript.html#jupyter-repl",
    "href": "notebooks/pyscript.html#jupyter-repl",
    "title": "PyScript",
    "section": "Jupyter REPL",
    "text": "Jupyter REPL\nYou can also have an entire Jupyter REPL running in your browser, with the ability to run arbitrary Python code!\nGo ahead and try this out:\nfrom matplotlib import pyplot as plt\nimport numpy as np\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale) * 500)\nax.set(title=\"Some random data!\")\nplt.show()"
  },
  {
    "objectID": "notebooks/brlda.html",
    "href": "notebooks/brlda.html",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "",
    "text": "Let‚Äôs start by loading the data and taking a look at it.\nimport pandas as pd\n\nemployees_df = pd.read_csv(\"../data/employees.csv\")\nsafehouses_df = pd.read_csv(\"../data/safehouses.csv\")\ndivisions_df = pd.read_csv(\"../data/divisions.csv\")\nmanagers_df = pd.read_csv(\"../data/managers.csv\")\nactions_df = pd.read_csv(\"../data/actions.csv\")"
  },
  {
    "objectID": "notebooks/brlda.html#employees",
    "href": "notebooks/brlda.html#employees",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Employees",
    "text": "Employees\n\nemployees_df.sample(5)\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n18217\n18221\nJonathan Barber\nData Analyst\njonathan_barber@brlda.gov\n(709)358-0654x450\nTheresa Rhodes\n\n\n25455\n25460\nLuis Hobbs\nScrum Master\nluis_hobbs@brlda.gov\n001-783-416-2922x9352\nDr. Charlene Hicks\n\n\n9952\n9955\nAdrienne Green\nData Analyst\nadrienne_green@brlda.gov\n(357)404-7351\nJasmine Ford\n\n\n9720\n9723\nStephen Smith\nScrum Master\nstephen_smith@brlda.gov\n250.989.9731x1477\nDonna Ross\n\n\n2487\n2489\nPatrick Benitez\nProject Manager\npatrick_benitez@brlda.gov\n\\t+1-087-924-1347x1681\nTerry Carroll\n\n\n\n\n\n\n\nIf they removed their users, we should see some missing EmployeeID values.\n\n# Check min and max EmployeeID\nmin_eid = employees_df[\"EmployeeID\"].min()\nmax_eid = employees_df[\"EmployeeID\"].max()\nprint(\"Min EmployeeID: \", min_eid)\nprint(\"Max EmployeeID: \", max_eid)\n\n# Check employees ids not in the min-max range\nmissing_employees = set(range(min_eid, max_eid + 1)) - set(employees_df[\"EmployeeID\"])\nprint(\"Employees ids not in the min-max range: \", missing_employees)\n\nMin EmployeeID:  1\nMax EmployeeID:  26849\nEmployees ids not in the min-max range:  {14976, 22602, 26188, 1423, 4284}\n\n\nWe should check that all managers are also employees\n\nmanagers = set(employees_df[\"Manager\"])\nemployees = set(employees_df[\"EmployeeName\"])\n\nmanagers - employees\n\nset()\n\n\nAre these ‚Äúmissing employees‚Äù in other tables?\n\ndivisions_df[divisions_df[\"EmployeeID\"].isin(missing_employees)]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n1422\n1423\nNaN\n[Division 7]\n[Project e-enable_holistic_models]\n[14, 214, 181, 219]\n\n\n4283\n4284\nNaN\n[Division 7]\n[Project repurpose_collaborative_methodologies...\n[10, 219]\n\n\n14975\n14976\nNaN\n[Division 7]\n[Project transform_24/365_functionalities]\n[25, 154, 231, 33, 219]\n\n\n22601\n22602\nNaN\n[Division 7]\n[Project monetize_one-to-one_mindshare, Projec...\n[12, 221, 19, 18, 219]\n\n\n26187\n26188\nNaN\n[Division 7]\n[Project extend_robust_action-items]\n[7, 219]\n\n\n\n\n\n\n\nWhat actions did they take?\n\nactions_df[actions_df[\"EmployeeID\"].isin(missing_employees)].sort_values(\"ActionDate\")[\n    \"ActionDescription\"\n].values\n\narray(['perform data mining on social media data for sentiment analysis through Advanced_coherent_architecture on Slovenia. Maintain strict confidentiality.',\n       'construct algorithms for automatic gait recognition through Phased_background_model on Guinea. Maintain strict confidentiality. During the covert operation, intercepted communications hinted at the presence of the three devices.',\n       'Initiate operation Networked_discrete_system_engine, targeting Yemen with objective to perform sentiment analysis on social media influencers In a confidential dossier, a defector mentioned the three devices being utilized to manipulate global financial markets.',\n       'Operation Re-contextualized_attitude-oriented_protocol to perform sentiment analysis on online news articles on North Macedonia is in progress. While examining classified documents, references to the three devices were discovered.',\n       \"construct algorithms for automatic vein recognition through Upgradable_incremental_application on Saint Pierre and Miquelon. Maintain strict confidentiality. A smuggler's confession revealed their involvement in transporting the three devices across international borders.\",\n       'Initiate operation Customizable_discrete_parallelism, targeting Costa Rica with objective to analyze temporal patterns in communication data',\n       'analyze communication patterns through Fully-configurable_6thgeneration_throughput on Chile. Maintain strict confidentiality. A recovered manuscript from an ancient civilization contained prophecies foretelling the rediscovery of the three devices in the modern era.',\n       'Operation Down-sized_24/7_capability to develop algorithms for automatic language translation on Argentina is in progress.',\n       'Initiate operation Centralized_upward-trending_concept, targeting Guinea with objective to develop algorithms for automatic language translation During an undercover operation, a suspect mentioned the existence of a secret society dedicated to harnessing the power of the three devices.',\n       \"Operation Digitized_methodical_structure to apply statistical analysis to identify patterns in data on Malta is in progress. A smuggler's confession revealed their involvement in transporting the three devices across international borders.\",\n       'Initiate operation Sharable_impactful_core, targeting Estonia with objective to develop algorithms for automatic target recognition Analysis of intercepted messages revealed a cryptic code mentioning the three devices.',\n       'analyze network topology for vulnerabilities through Enhanced_cohesive_application on Kuwait. Maintain strict confidentiality.',\n       \"utilize machine learning for anomaly detection through Ameliorated_global_superstructure on Nepal. Maintain strict confidentiality. A decrypted audio recording captured a high-level conversation detailing the three devices' ability to manipulate spacetime.\",\n       'perform text mining on classified documents through Multi-lateral_needs-based_Internet_solution on Aruba. Maintain strict confidentiality.',\n       'Initiate operation Digitized_dedicated_help-desk, targeting Bahamas with objective to create data-driven threat assessment models Analysis of surveillance footage identified an individual wearing a suspicious pendant believed to be related to the three devices.A reconnaissance team has been deployed around the Ecuador safehouse.',\n       'Operation Vision-oriented_explicit_flexibility to implement steganography techniques on Russian Federation is in progress. During the field investigation, a local informant reported sightings of individuals possessing the three devices.',\n       \"Initiate operation Realigned_exuding_Graphic_Interface, targeting Barbados with objective to analyze network traffic for web application attacks An encrypted email chain revealed a covert organization's plan to unleash the power of the three devices for their own agenda.\",\n       'Initiate operation Self-enabling_tangible_website, targeting Japan with objective to analyze temporal patterns in communication data Analysis of intercepted video footage revealed coded hand gestures used by a secret society to communicate about the three devices.'],\n      dtype=object)\n\n\nThey all mention the devices in their action description."
  },
  {
    "objectID": "notebooks/brlda.html#safehouses",
    "href": "notebooks/brlda.html#safehouses",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Safehouses",
    "text": "Safehouses\n\nsafehouses_df.sample(5)\n\n\n\n\n\n\n\n\nID\nCity\nAddress\nLatitude\nLongitude\n\n\n\n\n49\n150\nLondon\n13 Graemesdyke Avenue, London, SW14 7BH, Unite...\n51.465893\n-0.275055\n\n\n181\n8\nTokyo\n34L-16R, Tokyo Wangan Road, Haneda Kukou, Ota,...\n35.556423\n139.772161\n\n\n183\n7\nParis\n12 Rue Victor Hugo, 91390 Morsang-sur-Orge, Fr...\n48.664273\n2.357661\n\n\n165\n16\nMoscow\n—É–ª–∏—Ü–∞ –ì–ª–∏–Ω–∫–∏ 8, Firsanovka, Khimki, Moscow Obl...\n55.948711\n37.242120\n\n\n84\n83\nCartagena\nCentro, 472000 Cartagena, BOL, Colombia\n10.428231\n-75.570062\n\n\n\n\n\n\n\n\n# Map of safehouses with Latitude and Longitude\nimport folium\n\nsafehouses_map = folium.Map(\n    location=[safehouses_df[\"Latitude\"].mean(), safehouses_df[\"Longitude\"].mean()],\n    zoom_start=4,\n)\n\nfor index, row in safehouses_df.iterrows():\n    folium.Marker([row[\"Latitude\"], row[\"Longitude\"]], popup=row[\"ID\"]).add_to(\n        safehouses_map\n    )\n\nsafehouses_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nCute, but not very useful for now, let‚Äôs move to divisions."
  },
  {
    "objectID": "notebooks/brlda.html#divisions",
    "href": "notebooks/brlda.html#divisions",
    "title": "Bureau of Really Large Data Analysis Leaks",
    "section": "Divisions",
    "text": "Divisions\n\ndivisions_df\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n0\n1\nKelly Rios\n[Division 1]\n[Project deliver_visionary_web-readiness]\n[232, 1, 73, 217]\n\n\n1\n2\nMadison Barr\n[Division 6, Division 3]\n[Project repurpose_collaborative_methodologies]\n[192, 26, 118, 4]\n\n\n2\n3\nSue Anderson\n[Division 5, Division 1, Division 10]\n[Project repurpose_collaborative_methodologies]\n[19, 8, 130, 50]\n\n\n3\n4\nLaura Carlson\n[Division 9, Division 9, Division 2]\n[Project streamline_proactive_e-markets]\n[15, 232]\n\n\n4\n5\nCarrie Ali\n[Division 3, Division 6]\n[Project deliver_visionary_web-readiness]\n[158, 118]\n\n\n...\n...\n...\n...\n...\n...\n\n\n26844\n26845\nChristopher Riley\n[Division 8, Division 10, Division 5]\n[Project drive_value-added_mindshare, Project ...\n[226]\n\n\n26845\n26846\nEric Chan\n[Division 2, Division 6, Division 1]\n[Project deliver_visionary_web-readiness]\n[15, 99, 58]\n\n\n26846\n26847\nAmy Vazquez\n[Division 8, Division 10, Division 5]\n[Project embrace_transparent_networks, Project...\n[22]\n\n\n26847\n26848\nClifford Reyes\n[Division 9, Division 2, Division 6]\n[Project disintermediate_distributed_experienc...\n[98]\n\n\n26848\n26849\nNoah Snyder\n[Division 2, Division 9, Division 10]\n[Project strategize_value-added_bandwidth]\n[1]\n\n\n\n\n26849 rows √ó 5 columns\n\n\n\n\n# TODO: Explode Division and Project\n\n\nManagers\n\nmanagers_df\n\n\n\n\n\n\n\n\nManagerName\nEmployee_1\nEmployee_2\nEmployee_3\nEmployee_4\nEmployee_5\nEmployee_6\nEmployee_7\nEmployee_8\nEmployee_9\n...\nEmployee_20\nEmployee_21\nEmployee_22\nEmployee_23\nEmployee_24\nEmployee_25\nEmployee_26\nEmployee_27\nEmployee_28\nEmployee_29\n\n\n\n\n0\nRaven Price\nCarl Schmidt\nCristina Thompson\nMrs. Katherine Franklin\nBlake Garcia\nDrew Berger\nDave Dennis\nGary Castaneda\nJoan Crawford\nNicole Johnson\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nDavid Smith\nAnthony Flores\nJohn Trujillo\nElizabeth Page\nWilliam Hicks\nGail Salazar\nLinda Gonzalez\nChristopher Miller\nJames Beard\nBradley Rowe\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nMichelle Frazier\nCurtis Johnson\nMichael Hancock\nRenee Shaw\nLisa Lewis\nStephen Hunter\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nMelissa Conrad\nDenise Gibson PhD\nChristina Li\nRachel Snyder\nJames Taylor\nIvan Robles\nJulie Adams\nMichelle White\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nApril Martin\nRyan Wright\nDr. Philip Jordan\nGary Wolfe\nDeborah Green\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3830\nPeter Burns\nKristin Ballard\nLisa Leach\nMatthew Warner\nTeresa Alexander\nRobin Pierce\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3831\nRandall Martinez\nMaria Ortiz\nGeorge Graves\nAmy Baker\nDeanna Cole\nSamantha Collins\nMelissa Aguilar\nMichael Nguyen\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3832\nPeter Jackson\nChristopher Cook\nShannon Solis\nMatthew Davidson\nLorraine Moore\nGrant Garcia\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3833\nSusan Nguyen\nKevin Parks\nJames Holmes\nDanielle Long\nLynn Solomon\nJustin Fisher\nSarah Moran\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3834\nMary Castro\nMatthew Long\nJoshua Jones\nRachel Beasley\nWilliam Murphy\nSonya Knight\nKenneth Yang\nAlan Lopez\nHeidi Collins\nJacqueline Robinson\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3835 rows √ó 30 columns\n\n\n\n\n# Transform to long format\nclean_managers_df = managers_df.melt(\n    id_vars=[\"ManagerName\"], value_name=\"Employee\", var_name=\"EmployeeNumber\"\n)\n\n\nclean_managers_df\n\n\n\n\n\n\n\n\nManagerName\nEmployeeNumber\nEmployee\n\n\n\n\n0\nRaven Price\nEmployee_1\nCarl Schmidt\n\n\n1\nDavid Smith\nEmployee_1\nAnthony Flores\n\n\n2\nMichelle Frazier\nEmployee_1\nCurtis Johnson\n\n\n3\nMelissa Conrad\nEmployee_1\nDenise Gibson PhD\n\n\n4\nApril Martin\nEmployee_1\nRyan Wright\n\n\n...\n...\n...\n...\n\n\n111210\nPeter Burns\nEmployee_29\nNaN\n\n\n111211\nRandall Martinez\nEmployee_29\nNaN\n\n\n111212\nPeter Jackson\nEmployee_29\nNaN\n\n\n111213\nSusan Nguyen\nEmployee_29\nNaN\n\n\n111214\nMary Castro\nEmployee_29\nNaN\n\n\n\n\n111215 rows √ó 3 columns\n\n\n\n\nmanagers_df[managers_df[\"Employee_1\"].isna()]\n\n\n\n\n\n\n\n\nManagerName\nEmployee_1\nEmployee_2\nEmployee_3\nEmployee_4\nEmployee_5\nEmployee_6\nEmployee_7\nEmployee_8\nEmployee_9\n...\nEmployee_20\nEmployee_21\nEmployee_22\nEmployee_23\nEmployee_24\nEmployee_25\nEmployee_26\nEmployee_27\nEmployee_28\nEmployee_29\n\n\n\n\n3686\nJessica Stone\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1 rows √ó 30 columns\n\n\n\nJessica Stone is the only manager without any employees (unless there are NaNs gaps).\n\nclean_managers_df[clean_managers_df[\"Employee\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nManagerName\nEmployeeNumber\nEmployee\n\n\n\n\n17210\nChristopher Mckenzie\nEmployee_5\nJessica Stone\n\n\n\n\n\n\n\n\nemployees_df[employees_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n11560\n11563\nJessica Stone\nData Scientist\njessica_stone@brlda.gov\n001-234-563-9331\nChristopher Mckenzie\n\n\n\n\n\n\n\n\ndivisions_df[divisions_df[\"EmployeeName\"] == \"Jessica Stone\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nDivision\nProject\nknown_safehouses\n\n\n\n\n11562\n11563\nJessica Stone\n[Division 4]\n[Project mesh_cutting-edge_experiences, Projec...\n[18, 42, 44]\n\n\n\n\n\n\n\n\nclean_managers_df[clean_managers_df[\"Employee\"] == \"Christopher Mckenzie\"]\n\n\n\n\n\n\n\n\nManagerName\nEmployeeNumber\nEmployee\n\n\n\n\n21390\nKayla Lee\nEmployee_6\nChristopher Mckenzie\n\n\n\n\n\n\n\n\nemployees_df[employees_df[\"EmployeeName\"] == \"Christopher Mckenzie\"]\n\n\n\n\n\n\n\n\nEmployeeID\nEmployeeName\nJobTitle\nEmail\nPhone\nManager\n\n\n\n\n20679\n20683\nChristopher Mckenzie\nBusiness Analyst\nchristopher_mckenzie@brlda.gov\n3860450516\nKayla Lee"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nD A T A D E X\n",
    "section": "",
    "text": "Collaborate on Open Data using Open Source Tools\nDatadex links together tools and frameworks with the goal to allow everyone collaborate on Open Data like people collaborate on Open Source using the principles from the Open Data Stack.\nWith Datadex and the help of tools like dbt and DuckDB you can start modeling data by writing simple select statements!"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "\nD A T A D E X\n",
    "section": "üíª Usage",
    "text": "üíª Usage\nThis is an example of how you can use Datadex to model data, which is already configured with some sample datasets. Get things working end to end with the following steps:\n\nSetup dependencies with make deps.\nBuild your dbt models and save them to Parquet files with make run.\nExplore the data with make rill."
  },
  {
    "objectID": "index.html#what-can-you-do-with-datadex",
    "href": "index.html#what-can-you-do-with-datadex",
    "title": "\nD A T A D E X\n",
    "section": "üöÄ What can you do with Datadex?",
    "text": "üöÄ What can you do with Datadex?\n\nModel local and remote datasets with dbt.\nUse any of the other awesome dbt features like tests and docs. Docs are automatically generated and published on GitHub Pages."
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "\nD A T A D E X\n",
    "section": "‚öôÔ∏è Setup",
    "text": "‚öôÔ∏è Setup\nThe fastest way to start using Datadex is via VSCode Remote Containers. Once inside the develpment environment, you‚Äôll only need to run make deps.\n\n\n\n\n\nPS: The development environment can also run in your browser thanks to GitHub Codespaces."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "\nD A T A D E X\n",
    "section": "üéØ Motivation",
    "text": "üéØ Motivation\nThis small project was created after thinking how an Open Data Protocol could look like! I just wanted to stitch together a few open source technologies and see what could they do."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "\nD A T A D E X\n",
    "section": "üëè Acknowledgements",
    "text": "üëè Acknowledgements\n\nThis proof of concept was created thanks to open source projects like DuckDB and dbt.\nDatadex name was inspired by Juan Benet awesome data projects."
  }
]